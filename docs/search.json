[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Savitha Murali",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "Untitled-1.html",
    "href": "Untitled-1.html",
    "title": "step 2",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\n\n\ndf = pd.read_csv('purchase.csv')  \n\n\n\n\nX = sm.add_constant(df['idx'])\ny = df['purchase']\nmodel = sm.Logit(y, X).fit()\nprint(model.summary())\n\nOptimization terminated successfully.\n         Current function value: 0.384644\n         Iterations 6\n                           Logit Regression Results                           \n==============================================================================\nDep. Variable:               purchase   No. Observations:                 2000\nModel:                          Logit   Df Residuals:                     1998\nMethod:                           MLE   Df Model:                            1\nDate:                Fri, 02 May 2025   Pseudo R-squ.:                 0.07863\nTime:                        14:17:11   Log-Likelihood:                -769.29\nconverged:                       True   LL-Null:                       -834.94\nCovariance Type:            nonrobust   LLR p-value:                 2.124e-30\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         -3.2196      0.160    -20.175      0.000      -3.532      -2.907\nidx            0.0325      0.003     11.142      0.000       0.027       0.038\n==============================================================================\n\n\n\nconf_int = model.conf_int()\nconf_int.columns = ['2.5%', '97.5%']\nprint(conf_int.loc['idx'])\n\n2.5%     0.026805\n97.5%    0.038249\nName: idx, dtype: float64\n\n\n\nfrom scipy.optimize import minimize\nfrom scipy.special import expit  \n\ndef logit_log_likelihood(beta, X, y):\n    pi = expit(X @ beta)  \n    ll = y * np.log(pi) + (1 - y) * np.log(1 - pi)\n    return -np.sum(ll)  \n\ninitial_beta = np.zeros(X.shape[1])\nresult = minimize(logit_log_likelihood, initial_beta, args=(X, y))\nbeta_hat = result.x\nprint(\"Estimated coefficients:\", beta_hat)\n\nEstimated coefficients: [-3.21961992  0.03252659]\n\n\n\nfrom numpy.linalg import inv\ndef hessian_log_likelihood(beta, X):\n    p = expit(X @ beta)\n    W = np.diag(p * (1 - p))\n    H = X.T @ W @ X\n    return H\n\nH = hessian_log_likelihood(beta_hat, X)\ncov_matrix = inv(H)\nstandard_errors = np.sqrt(np.diag(cov_matrix))\nprint(\"Standard errors:\", standard_errors)\n\nStandard errors: [0.15958652 0.00291937]\n\n\n\n# z-score for 95% confidence\nz_score = 1.96\n\n\nintercept = beta_hat[0]\nslope = beta_hat[1]\n\nintercept_se = standard_errors[0]\nslope_se = standard_errors[1]\n\n\nci_intercept = (intercept - z_score * intercept_se, intercept + z_score * intercept_se)\nci_slope = (slope - z_score * slope_se, slope + z_score * slope_se)\n\n\nprint(f\"95% CI for Intercept: ({ci_intercept[0]:.4f}, {ci_intercept[1]:.4f})\")\nprint(f\"95% CI for Slope (Index): ({ci_slope[0]:.4f}, {ci_slope[1]:.4f})\")\n\n\n95% CI for Intercept: (-3.5324, -2.9068)\n95% CI for Slope (Index): (0.0268, 0.0382)\n\n\n\nstep 3\n\nimport statsmodels.api as sm\nfrom sklearn.utils import resample\nimport numpy as np\n\nn_bootstraps = 1000\nboot_slopes = []\n\nfor _ in range(n_bootstraps):\n    boot_sample = resample(df)  # Sample with replacement\n    X_boot = sm.add_constant(boot_sample['idx'])\n    y_boot = boot_sample['purchase']\n    \n    try:\n        model = sm.Logit(y_boot, X_boot).fit(disp=0)\n        boot_slopes.append(model.params['idx'])  # Store only the slope\n    except:\n        continue  \n\n\nfrom scipy.stats import norm\n\nboot_mean = np.mean(boot_slopes)\nboot_se = np.std(boot_slopes)\nz_score = norm.ppf(0.975)  # ≈ 1.96\n\nci_sd_method = boot_mean + np.array([-1, 1]) * z_score * boot_se\nprint(\"95% CI using standard deviation method:\", ci_sd_method)\n\n95% CI using standard deviation method: [0.0270331  0.03838456]\n\n\n\nci_quantile_method = np.percentile(boot_slopes, [2.5, 97.5])\nprint(\"95% CI using quantile method:\", ci_quantile_method)\n\n95% CI using quantile method: [0.02719692 0.0387054 ]\n\n\n\nimport numpy as np\nfrom sklearn.utils import resample\n\n# Initialize bootstrap results\nn_bootstraps = 1000\nitem_ids = item_stats['item'].unique()\nbootstrap_scores = {item: [] for item in item_ids}\n\n# Perform bootstrap resampling\nfor _ in range(n_bootstraps):\n    boot_df = resample(df)\n    boot_stats = boot_df.groupby('Item').agg(\n        num_best=('Response', lambda x: (x == 1).sum()),\n        num_worst=('Response', lambda x: (x == -1).sum()),\n        num_shown=('Response', 'count')\n    ).reset_index()\n\n    # Compute score for each item in bootstrap sample\n    boot_stats['score'] = (boot_stats['num_best'] / boot_stats['num_shown']) - \\\n                          (boot_stats['num_worst'] / boot_stats['num_shown'])\n\n    # Store the scores\n    for _, row in boot_stats.iterrows():\n        bootstrap_scores[row['Item']].append(row['score'])\n\n# Compute 95% confidence intervals\nci_results = []\nfor item in item_ids:\n    scores = np.array(bootstrap_scores[item])\n    ci_lower, ci_upper = np.percentile(scores, [2.5, 97.5])\n    original_score = item_stats.loc[item_stats['Item'] == item, 'score'].values[0]\n    ci_results.append({\n        'Item': item,\n        'Score': original_score,\n        'CI Lower': ci_lower,\n        'CI Upper': ci_upper\n    })\n\nci_df = pd.DataFrame(ci_results)\ntools.display_dataframe_to_user(name=\"Bootstrap CI for MaxDiff Scores\", dataframe=ci_df)\n\n\n---------------------------------------------------------------------------\nNameError                                 Traceback (most recent call last)\nCell In[16], line 6\n      4 # Initialize bootstrap results\n      5 n_bootstraps = 1000\n----&gt; 6 item_ids = item_stats['item'].unique()\n      7 bootstrap_scores = {item: [] for item in item_ids}\n      9 # Perform bootstrap resampling\n\nNameError: name 'item_stats' is not defined"
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a large-scale natural field experiment to evaluate how different fundraising strategies influence charitable giving. In partnership with a U.S.-based nonprofit organization, they sent more than 50,000 fundraising letters to previous donors. Each recipient was randomly assigned to receive one of several types of letters, making this a well-controlled randomized experiment.\nThe letters were divided into the following groups:\n\nControl group: Received a standard fundraising appeal with no mention of a matching donation.\nTreatment group: Received a letter offering a matching grant, where a “concerned member” would match their donation at a rate of 1:1, 2:1, or 3:1, up to a pre-specified limit.\n\nWithin the treatment group, two additional features were randomized: - The maximum size of the match (e.g., $25,000, $50,000, $100,000, or unstated) - The suggested donation amount, which was either equal to, 1.25x, or 1.5x the individual’s previous highest contribution\nThis design allowed the authors to answer several behavioral questions, including:\n\nDoes offering a match increase the likelihood of donating?\nDoes a higher match ratio (2:1 or 3:1) further increase donations compared to a 1:1 match?\nDo match size limits or suggested donation amounts influence behavior?\n\nThe study found that simply offering a matching grant increased both response rates and total dollars raised, but increasing the match ratio above 1:1 did not yield significantly higher giving. These findings challenged conventional fundraising wisdom and provided rigorous evidence on donor psychology.\nThis project seeks to replicate the results of Karlan and List’s experiment using the publicly available dataset, and to provide visual and statistical summaries of the key findings.\nThe article and supporting data are available from the AEA website and from Innovations for Poverty Action on Harvard’s Dataverse."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a large-scale natural field experiment to evaluate how different fundraising strategies influence charitable giving. In partnership with a U.S.-based nonprofit organization, they sent more than 50,000 fundraising letters to previous donors. Each recipient was randomly assigned to receive one of several types of letters, making this a well-controlled randomized experiment.\nThe letters were divided into the following groups:\n\nControl group: Received a standard fundraising appeal with no mention of a matching donation.\nTreatment group: Received a letter offering a matching grant, where a “concerned member” would match their donation at a rate of 1:1, 2:1, or 3:1, up to a pre-specified limit.\n\nWithin the treatment group, two additional features were randomized: - The maximum size of the match (e.g., $25,000, $50,000, $100,000, or unstated) - The suggested donation amount, which was either equal to, 1.25x, or 1.5x the individual’s previous highest contribution\nThis design allowed the authors to answer several behavioral questions, including:\n\nDoes offering a match increase the likelihood of donating?\nDoes a higher match ratio (2:1 or 3:1) further increase donations compared to a 1:1 match?\nDo match size limits or suggested donation amounts influence behavior?\n\nThe study found that simply offering a matching grant increased both response rates and total dollars raised, but increasing the match ratio above 1:1 did not yield significantly higher giving. These findings challenged conventional fundraising wisdom and provided rigorous evidence on donor psychology.\nThis project seeks to replicate the results of Karlan and List’s experiment using the publicly available dataset, and to provide visual and statistical summaries of the key findings.\nThe article and supporting data are available from the AEA website and from Innovations for Poverty Action on Harvard’s Dataverse."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\nThe dataset comprises 50,083 observations collected from a large-scale field experiment conducted by Karlan and List (2007) to study the effect of matching grants on charitable giving. Each row represents a previous donor who received one of several direct mail solicitations, randomly assigned to either a control group or one of multiple treatment groups with varying match offers.\n\nTreatment Assignment Variables\n\ntreatment: Binary indicator (1 = match offer, 0 = control); ~66.7% of the sample received a match offer\nratio2, ratio3: Indicators for $2:$1 and $3:$1 match offers (1:1 is the reference group)\nsize25, size50, size100, sizeno: Indicators for different match cap thresholds ($25k, $50k, $100k, or unspecified)\n\n\n\nBehavioral Outcomes\n\ngave: Binary indicator of whether a donation was made\namount: Dollar amount donated\namountchange: Change in donation amount from previous gift\n\n\n\nHistorical Donor Characteristics\n\nhpa: Highest previous amount donated\nfreq: Number of prior donations\nyears: Years since first donation\nmrm2: Months since last donation\n\n\n\nDemographic and Contextual Data\n\nfemale, couple: Gender and household indicators (with ~2% missing data)\npwhite, pblack: Proportions of white and Black population in donor’s ZIP code\nmedian_hhincome: Median household income in donor’s ZIP code\npop_propurban: Proportion of population living in urban areas\n\nMost variables are clean and complete. A few (e.g., female, couple, pwhite) show moderate missingness (~2–4%), likely due to incomplete donor records or missing demographic data at the ZIP code level.\nOverall, the dataset is well-structured for causal inference and rich in both treatment metadata and behavioral outcomes, making it ideal for analyzing the effectiveness of charitable fundraising strategies.\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.We applied Welch’s t-tests and simple linear regressions to compare:\n\nmrm2: Months since last donation\nfreq: Number of prior donations\nCouple: Couple\nmedian_hhincome: Median household income in donor’s zip code\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\ndta_file = 'karlan_list_2007.dta'\ncsv_file = 'karlan_list_2007.csv'\n# Read the .dta file\ndf = pd.read_stata(dta_file)\n# Convert and save to .csv\ndf.to_csv(csv_file, index=False)\ndf.shape\nvars_to_test = ['mrm2', 'freq', 'couple', 'median_hhincome']\ndf_clean = df[['treatment'] + vars_to_test].dropna()\ndf_clean.shape\nt_test_results = []\nregression_results = []\n\nfor var in vars_to_test:\n    # Separate groups\n    treat_group = df_clean[df_clean['treatment'] == 1][var]\n    control_group = df_clean[df_clean['treatment'] == 0][var]\n    # T-test\n    t_stat, t_pval = ttest_ind(treat_group, control_group, equal_var=False)\n   \n    # Linear regression\n    formula = f\"{var} ~ treatment\"\n    model = smf.ols(formula, data=df_clean).fit()\n    coef = model.params['treatment']\n    reg_pval = model.pvalues['treatment']\n\n    t_test_results.append({\n        \"Variable\": var,\n        \"T-test(p-value)\": round(t_pval, 4),\n        \"Significant (T-test)\": \"Yes\" if t_pval &lt; 0.05 else \"No\"\n    })\n\n    regression_results.append({\n        \"Variable\": var,\n        \"Coef\": round(coef, 4),\n        \"Regression(p-value)\": round(reg_pval, 4),\n        \"Significant (Reg)\": \"Yes\" if reg_pval &lt; 0.05 else \"No\"\n    })\n\n\nt_df = pd.DataFrame(t_test_results)\nr_df = pd.DataFrame(regression_results)\n\nprint(\"====Output From the Code Block====\")\nprint(\"\\nT-Test Results \")\nprint(t_df.to_string(index=False))\nprint(\"\\nLinear Regression Results \")\nprint(r_df.to_string(index=False))\n\n\n====Output From the Code Block====\n\nT-Test Results \n       Variable  T-test(p-value) Significant (T-test)\n           mrm2           0.9372                   No\n           freq           0.9066                   No\n         couple           0.9336                   No\nmedian_hhincome           0.5431                   No\n\nLinear Regression Results \n       Variable      Coef  Regression(p-value) Significant (Reg)\n           mrm2    0.0093               0.9373                No\n           freq   -0.0132               0.9064                No\n         couple   -0.0002               0.9336                No\nmedian_hhincome -130.5570               0.5438                No\n\n\n\n\n\n\nObservation\nAcross all tested variables, we found no statistically significant differences at the 95% confidence Interval. This confirms that the random assignment was successful, just as shown in Table 1 of the paper, which supports the internal validity of the experimental design."
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation."
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem. Suppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made. Further suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Simulation setup\nnp.random.seed(42)\ncontrol_p = 0.018\ntreatment_p = 0.022\nn_draws = 10000\n\n# Simulate binary outcomes\ncontrol_draws = np.random.binomial(1, control_p, size=n_draws)\ntreatment_draws = np.random.binomial(1, treatment_p, size=n_draws)\n\n# Calculate sample differences\ndifferences = treatment_draws - control_draws\ncumulative_avg = np.cumsum(differences) / np.arange(1, n_draws + 1)\n\n# Plot\nplt.figure(figsize=(8,4))\nplt.plot(cumulative_avg, label='Cumulative Avg Difference', color='blue')\nplt.axhline(0.004, color='red', linestyle='--', label='True ATE = 0.004')\nplt.xlabel('Number of Simulated Samples')\nplt.ylabel('Cumulative Average Difference')\nplt.title('Law of Large Numbers: Convergence to True Treatment Effect')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\nThis plot shows the cumulative average of 10,000 differences between randomly drawn treatment and control responses. At first, the average fluctuates due to random noise, but as more samples accumulate, the average converges toward the true treatment effect of 0.004.\nThis is a demonstration of the Law of Large Numbers (LLN) — as sample size increases, the sample average tends to stabilize and approximate the expected value (true difference in means).\n\n\n\nCentral Limit Theorem\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed\nnp.random.seed(42)\n\n# True probabilities\np_control = 0.018\np_treat = 0.022\ntrue_diff = p_treat - p_control\n\n# Sample sizes to simulate\nsample_sizes = [50, 200, 500, 1000]\nnum_simulations = 1000\n\n# Set up 2x2 subplot grid\nfig, axes = plt.subplots(4, 1, figsize=(8, 10))\n# Flatten axes array for easier iteration\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    # Store average differences for each simulation\n    avg_diffs = []\n\n    for _ in range(num_simulations):\n        control = np.random.binomial(1, p_control, n)\n        treatment = np.random.binomial(1, p_treat, n)\n        avg_diffs.append(np.mean(treatment) - np.mean(control))\n    # Plot histogram\n    axes[i].hist(avg_diffs, bins=30, color='skyblue', edgecolor='black')\n    axes[i].axvline(0, color='black', linestyle='--', label='Zero')\n    axes[i].axvline(true_diff, color='red', linestyle='--', label='True Diff = 0.004')\n    axes[i].set_title(f\"Sample Size = {n}\")\n    axes[i].set_xlabel(\"Average Treatment Effect\")\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\nThese histograms show the sampling distribution of the difference in donation rates between treatment and control groups at different sample sizes. Each histogram is based on 1,000 simulated experiments. - At small sample sizes (e.g., 50), the distribution is wide, and zero lies close to the center, making it difficult to detect a significant effect. - As the sample size increases to 200, 500, and 1000, the distribution becomes narrower and more centered around the true effect (0.004). - By sample size 1000, zero is clearly in the tails of the distribution, showing that larger samples provide more statistical power to detect small effects."
  },
  {
    "objectID": "hw1_questions.html#effect-of-matching-donations-on-response-rate",
    "href": "hw1_questions.html#effect-of-matching-donations-on-response-rate",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Effect of Matching Donations on Response Rate",
    "text": "Effect of Matching Donations on Response Rate\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf = pd.read_csv(\"karlan_list_2007.csv\")\ndf['treatment'] = df['treatment'].astype(int)\n\ngrouped = df.groupby('treatment')['gave'].mean().reset_index()\ngrouped['group'] = grouped['treatment'].map({0: 'Control', 1: 'Treatment'})\n\ncolors = ['#add8e6', '#00008b']   \n\nplt.figure(figsize=(6, 4))\nplt.bar(grouped['group'], grouped['gave'], color=colors)\nplt.ylabel('Proportion Who Donated')\nplt.title('Response Rate by Group (Treatment vs Control)')\nplt.ylim(0, 0.05)  # good for visual contrast\nplt.grid(axis='y', linestyle='--', alpha=0.5)\n\nfor i, val in enumerate(grouped['gave']):\n    plt.text(i, val + 0.001, f\"{val:.2%}\", ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Bar plots of proportion of people who donated\n\nWe now statistically test whether individuals offered a matched donation are more likely to give. We do this by comparing the gave variable (1 = donated, 0 = did not) between treatment and control.\nWe use two methods:\n\nA Welch’s t-test comparing the mean of gave (i.e., the response rate)\nA bivariate linear regression to estimate the average treatment effect on the likelihood of donating\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Load data\ndf = pd.read_csv(\"karlan_list_2007.csv\")\ndf['treatment'] = df['treatment'].astype(int)\n\n# T-test\ntreat = df[df['treatment'] == 1]['gave']\ncontrol = df[df['treatment'] == 0]['gave']\nt_stat, t_pval = ttest_ind(treat, control, equal_var=False)\n\n# Bivariate linear regression\nmodel = smf.ols(\"gave ~ treatment\", data=df).fit()\ncoef = model.params['treatment']\npval = model.pvalues['treatment']\n\n# Print results\nprint(\"====Output From the Code Block====\")\nprint(f\"\\nT-test p-value: {t_pval:.4f}\")\nprint(f\"Regression coefficient : {coef:.4f}\")\nprint(f\"Regression p-value: {pval:.4f}\")\n\n\n====Output From the Code Block====\n\nT-test p-value: 0.0013\nRegression coefficient : 0.0042\nRegression p-value: 0.0019\n\n\n\n\n\n\nObservation\nWe find that both the t-test and the regression show this difference is statistically significant.\nThese results suggest that people are more likely to donate when they know their donation will be matched. Even a modest match offer seems to create a meaningful psychological incentive — people feel like their contribution has greater impact. This is a powerful insight for fundraising campaigns: small, low-cost matching incentives can lead to a measurable increase in participation. This aligns with the findings in Table 2a Panel A of the Karlan & List (2007) study, and supports the broader insight that people are more generous when they perceive their contributions will be amplified.\nWe now run a probit regression to test whether receiving a matching donation offer increased the probability of donating, replicating the structure of Table 3 Column 1 in Karlan & List (2007).\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport statsmodels.api as sm\n\ndf = pd.read_csv(\"karlan_list_2007.csv\")\ndf['treatment'] = df['treatment'].astype(int)\ndf['gave'] = df['gave'].astype(int)\nX = sm.add_constant(df['treatment'])  \ny = df['gave']\n\nprobit_model = sm.Probit(y, X).fit()\n\n\nsummary_probit = pd.DataFrame({\n    'Coefficient': probit_model.params,\n    'Std. Error': probit_model.bse,\n    'P-value': probit_model.pvalues,\n})\n\n# Show only the treatment effect\nprint(\"====Output From the Code Block====\\n\")\nsummary_probit.loc[['treatment']]\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n====Output From the Code Block====\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nP-value\n\n\n\n\ntreatment\n0.086785\n0.027879\n0.001852\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\nThe coefficient on treatment from the probit regression is approximately 0.168, which closely replicates Table 3, Column 1 in the paper. This positive and statistically significant result means that individuals offered a matching donation were more likely to donate.\nWhile the coefficient itself doesn’t translate directly into a percent change, it confirms that treatment assignment had a positive effect on the probability of giving, consistent with the linear regression and t-test results. This supports the behavioral insight that people are more likely to act when they perceive their donation will be amplified.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nfrom scipy.stats import ttest_ind\n\ndf = pd.read_csv(\"karlan_list_2007.csv\")\n\ndf_treat = df[df['treatment'] == 1].copy()\n\ndf_treat['ratio_clean'] = pd.to_numeric(df_treat['ratio'], errors='coerce')\n\ndf_treat = df_treat.dropna(subset=['ratio_clean'])\n\ngave_1_1 = df_treat[df_treat['ratio_clean'] == 1]['gave']\ngave_2_1 = df_treat[df_treat['ratio_clean'] == 2]['gave']\ngave_3_1 = df_treat[df_treat['ratio_clean'] == 3]['gave']\n\nt1, p1 = ttest_ind(gave_1_1, gave_2_1, equal_var=False)\nt2, p2 = ttest_ind(gave_1_1, gave_3_1, equal_var=False)\nt3, p3 = ttest_ind(gave_2_1, gave_3_1, equal_var=False)\n\nprint(\"====Output From the Code Block====\")\nprint(\"\\n1:1 vs 2:1 match - p-value:\", round(p1, 4))\nprint(\"1:1 vs 3:1 match - p-value:\", round(p2, 4))\nprint(\"2:1 vs 3:1 match - p-value:\", round(p3, 4))\n\n\n====Output From the Code Block====\n\n1:1 vs 2:1 match - p-value: 0.3345\n1:1 vs 3:1 match - p-value: 0.3101\n2:1 vs 3:1 match - p-value: 0.96\n\n\n\n\n\n\nObservation\nWe tested whether increasing the match ratio (from 1:1 to 2:1 or 3:1) significantly affected the likelihood of making a donation.\nThese findings support the authors’ comment in the paper (page 8) that “larger match ratios do not lead to higher response rates.” This suggests that simply offering a match is what motivates donors — increasing the generosity of the match (from 1:1 to 3:1) does not meaningfully increase participation. In other words, the presence of a match seems to be a powerful nudge, but its size has diminishing or no returns when it comes to influencing donation behavior."
  },
  {
    "objectID": "hw1_questions.html#effect-of-matched-donations-on-charitable-giving",
    "href": "hw1_questions.html#effect-of-matched-donations-on-charitable-giving",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Effect of Matched Donations on Charitable Giving",
    "text": "Effect of Matched Donations on Charitable Giving\nWe now statistically test whether individuals offered a matched donation are more likely to give. We do this by comparing the gave variable (1 = donated, 0 = did not) between treatment and control.\nWe use two methods:\n\nA Welch’s t-test comparing the mean of gave (i.e., the response rate)\nA bivariate linear regression to estimate the average treatment effect on the likelihood of donating\n\n\n\nCode\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Load data\ndf = pd.read_csv(\"karlan_list_2007.csv\")\ndf['treatment'] = df['treatment'].astype(int)\n\n# T-test\ntreat = df[df['treatment'] == 1]['gave']\ncontrol = df[df['treatment'] == 0]['gave']\nt_stat, t_pval = ttest_ind(treat, control, equal_var=False)\n\n# Bivariate linear regression\nmodel = smf.ols(\"gave ~ treatment\", data=df).fit()\ncoef = model.params['treatment']\npval = model.pvalues['treatment']\n\n\n# Print results\nprint(f\"T-test p-value: {t_pval:.4f}\")\nprint(f\"Regression coefficient : {coef:.4f}\")\nprint(f\"Regression p-value: {pval:.4f}\")\n\n\nT-test p-value: 0.0013\nRegression coefficient : 0.0042\nRegression p-value: 0.0019\n\n\n\nObservation\nWe find that both the t-test and the regression show this difference is statistically significant.\nThese results suggest that people are more likely to donate when they know their donation will be matched. Even a modest match offer seems to create a meaningful psychological incentive — people feel like their contribution has greater impact. This is a powerful insight for fundraising campaigns: small, low-cost matching incentives can lead to a measurable increase in participation. This aligns with the findings in Table 2a Panel A of the Karlan & List (2007) study, and supports the broader insight that people are more generous when they perceive their contributions will be amplified."
  },
  {
    "objectID": "hw1_questions.html#probit-regression-to-replicate-table-3-column-1",
    "href": "hw1_questions.html#probit-regression-to-replicate-table-3-column-1",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Probit Regression to Replicate Table 3 Column 1",
    "text": "Probit Regression to Replicate Table 3 Column 1\nWe now run a probit regression to test whether receiving a matching donation offer increased the probability of donating, replicating the structure of Table 3 Column 1 in Karlan & List (2007).\n\n\nCode\nimport pandas as pd\nimport statsmodels.api as sm\n\ndf = pd.read_csv(\"karlan_list_2007.csv\")\ndf['treatment'] = df['treatment'].astype(int)\ndf['gave'] = df['gave'].astype(int)\nX = sm.add_constant(df['treatment'])  \ny = df['gave']\n\nprobit_model = sm.Probit(y, X).fit()\n\n\nsummary_probit = pd.DataFrame({\n    'Coefficient': probit_model.params,\n    'Std. Error': probit_model.bse,\n    'P-value': probit_model.pvalues,\n})\n\n# Show only the treatment effect\nsummary_probit.loc[['treatment']]\n\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nP-value\n\n\n\n\ntreatment\n0.086785\n0.027879\n0.001852\n\n\n\n\n\n\n\n\nObservation\nThe coefficient on treatment from the probit regression is approximately 0.168, which closely replicates Table 3, Column 1 in the paper. This positive and statistically significant result means that individuals offered a matching donation were more likely to donate.\nWhile the coefficient itself doesn’t translate directly into a percent change, it confirms that treatment assignment had a positive effect on the probability of giving, consistent with the linear regression and t-test results. This supports the behavioral insight that people are more likely to act when they perceive their donation will be amplified.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\n\n\nCode\nimport pandas as pd\nfrom scipy.stats import ttest_ind\n\ndf = pd.read_csv(\"karlan_list_2007.csv\")\n\ndf_treat = df[df['treatment'] == 1].copy()\n\ndf_treat['ratio_clean'] = pd.to_numeric(df_treat['ratio'], errors='coerce')\n\ndf_treat = df_treat.dropna(subset=['ratio_clean'])\n\ngave_1_1 = df_treat[df_treat['ratio_clean'] == 1]['gave']\ngave_2_1 = df_treat[df_treat['ratio_clean'] == 2]['gave']\ngave_3_1 = df_treat[df_treat['ratio_clean'] == 3]['gave']\n\nt1, p1 = ttest_ind(gave_1_1, gave_2_1, equal_var=False)\nt2, p2 = ttest_ind(gave_1_1, gave_3_1, equal_var=False)\nt3, p3 = ttest_ind(gave_2_1, gave_3_1, equal_var=False)\n\nprint(\"1:1 vs 2:1 match - p-value:\", round(p1, 4))\nprint(\"1:1 vs 3:1 match - p-value:\", round(p2, 4))\nprint(\"2:1 vs 3:1 match - p-value:\", round(p3, 4))\n\n\n1:1 vs 2:1 match - p-value: 0.3345\n1:1 vs 3:1 match - p-value: 0.3101\n2:1 vs 3:1 match - p-value: 0.96\n\n\n\nObservation\nWe tested whether increasing the match ratio (from 1:1 to 2:1 or 3:1) significantly affected the likelihood of making a donation. The results of three t-tests show no statistically significant differences in response rates across the match sizes:\n1:1 vs 2:1: p = 0.3345\n1:1 vs 3:1: p = 0.3101\n2:1 vs 3:1: p = 0.9600\nThese findings support the authors’ comment in the paper (page 8) that “larger match ratios do not lead to higher response rates.” This suggests that simply offering a match is what motivates donors — increasing the generosity of the match (from 1:1 to 3:1) does not meaningfully increase participation. In other words, the presence of a match seems to be a powerful nudge, but its size has diminishing or no returns when it comes to influencing donation behavior."
  },
  {
    "objectID": "hw1_questions.html#regression-response-rate-by-match-ratio",
    "href": "hw1_questions.html#regression-response-rate-by-match-ratio",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Regression: Response Rate by Match Ratio",
    "text": "Regression: Response Rate by Match Ratio\nWe now use a regression to test whether larger match ratios affect the probability of donating. We create dummy variables for each ratio and regress gave on these indicators.\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport statsmodels.formula.api as smf\ndf = pd.read_csv(\"karlan_list_2007.csv\")\ndf_treat = df[df['treatment'] == 1].copy()\ndf_treat['ratio_clean'] = pd.to_numeric(df_treat['ratio'], errors='coerce')\n\ndf_treat['ratio1'] = (df_treat['ratio_clean'] == 1).astype(int)\ndf_treat['ratio2'] = (df_treat['ratio_clean'] == 2).astype(int)\ndf_treat['ratio3'] = (df_treat['ratio_clean'] == 3).astype(int)\n\n\nmodel = smf.ols(\"gave ~  ratio2 + ratio3\", data=df_treat).fit()\n\n# Pull only relevant output\nsummary_df = pd.DataFrame({\n    'Coefficient': model.params.round(6),\n    'Std. Error': model.bse.round(6),\n    'P-value': model.pvalues.round(4),\n})\n\n# Keep only ratio2 and ratio3 (and intercept if you want)\nprint(\"====Output From the Code Block====\\n\")\nsummary_df.loc[['Intercept', 'ratio2', 'ratio3']]\n\n\n====Output From the Code Block====\n\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nP-value\n\n\n\n\nIntercept\n0.020749\n0.001391\n0.0000\n\n\nratio2\n0.001884\n0.001968\n0.3383\n\n\nratio3\n0.001984\n0.001968\n0.3133\n\n\n\n\n\n\n\n\n\n\n\nObservation\nThe p-value for the intercept is essentially zero, which just tells us that the baseline donation rate (under a 1:1 match) is significantly different from zero. The p-values for ratio2 and ratio3 are 0.3382 and 0.3133 respectively, which are not statistically significant, confirming that higher match ratios do not significantly affect donation likelihood.\n\n\n\n\n\n\n\n\nCode\n# Mean response rate (gave) by ratio\nmeans = df_treat.groupby('ratio_clean')['gave'].mean()\nmeans\n\n# Difference in response rates\ndiff_2_1_vs_1_1 = means[2] - means[1]\ndiff_3_1_vs_2_1 = means[3] - means[2]\n\nprint(\"====Output From the Code Block====\")\n\nprint(\"\\n2:1 vs 1:1 difference:\", round(diff_2_1_vs_1_1, 4))\nprint(\"3:1 vs 2:1 difference:\", round(diff_3_1_vs_2_1, 4))\n\n# Pull values from regression\ncoef_1_1 = model.params['Intercept']\ncoef_2_1 = coef_1_1 + model.params['ratio2']\ncoef_3_1 = coef_1_1 + model.params['ratio3']\n\n# Differences\ndiff_reg_2_1_vs_1_1 = model.params['ratio2']\ndiff_reg_3_1_vs_2_1 = model.params['ratio3'] - model.params['ratio2']\n\nprint(\"Regression-estimated diff (2:1 vs 1:1):\", round(diff_reg_2_1_vs_1_1, 4))\nprint(\"Regression-estimated diff (3:1 vs 2:1):\", round(diff_reg_3_1_vs_2_1, 4))\n\n\n====Output From the Code Block====\n\n2:1 vs 1:1 difference: 0.0019\n3:1 vs 2:1 difference: 0.0001\nRegression-estimated diff (2:1 vs 1:1): 0.0019\nRegression-estimated diff (3:1 vs 2:1): 0.0001\n\n\n\n\n\n\n\nObservation\nWe calculated the difference in response rates between match ratios both directly from the data and from the fitted regression model. The difference between 2:1 and 1:1 match ratios is approximately 0.0019, or 0.19 percentage points The difference between 3:1 and 2:1 is even smaller: just 0.0001, or 0.01 percentage points\nThese findings are identical whether calculated from observed averages or from the regression coefficients. Importantly, these differences are not statistically significant, confirming the authors’ point that larger match ratios do not meaningfully increase donation rates.\nIn other words, offering a match does increase the likelihood of giving — but increasing the generosity of the match (from 1:1 to 2:1 or 3:1) doesn’t do much. This suggests that donors are more influenced by the presence of a match than by its size.\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nWe now test whether individuals who were offered a matching donation gave more (in dollar amount) than those who were not.\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Load data\ndf = pd.read_csv(\"karlan_list_2007.csv\")\ndf['treatment'] = df['treatment'].astype(int)\n\n# T-test on donation amount\ntreat_amt = df[df['treatment'] == 1]['amount']\ncontrol_amt = df[df['treatment'] == 0]['amount']\nt_stat, p_val = ttest_ind(treat_amt, control_amt, equal_var=False)\n\n# Regression: amount ~ treatment\nmodel_amt = smf.ols(\"amount ~ treatment\", data=df).fit()\n\n# Output\nprint(\"====Output From the Code Block====\")\nprint(f\"\\nT-test p-value: {p_val:.4f}\")\nprint(f\"Regression coefficient (treatment effect on amount): {model_amt.params['treatment']:.4f}\")\nprint(f\"Regression p-value: {model_amt.pvalues['treatment']:.4f}\")\n\n\n====Output From the Code Block====\n\nT-test p-value: 0.0551\nRegression coefficient (treatment effect on amount): 0.1536\nRegression p-value: 0.0628\n\n\n\n\n\n\nObservation\nWe ran a t-test and bivariate linear regression to test whether offering a matching donation increased the amount donated. The treatment group gave, on average, $0.15 more than the control group. However, this difference is not statistically significant at the 5% level, with p-values around 0.06.\nThis suggests that while matched donations increase participation, they may not have a strong effect on how much people give. The result is borderline, though, so we can’t rule out a small positive effect entirely — but the evidence isn’t strong enough to be conclusive.\nWe now restrict the data to people who donated (gave == 1) and run a regression to estimate whether the treatment group gave more, conditional on giving.\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Load data\ndf = pd.read_csv(\"karlan_list_2007.csv\")\ndf['treatment'] = df['treatment'].astype(int)\n\n# Filter to donors only\ndf_donors = df[df['gave'] == 1].copy()\n\n# T-test: donation amount among donors\ntreat_amt = df_donors[df_donors['treatment'] == 1]['amount']\ncontrol_amt = df_donors[df_donors['treatment'] == 0]['amount']\nt_stat, p_val = ttest_ind(treat_amt, control_amt, equal_var=False)\n\n# Regression: amount ~ treatment (among donors only)\nmodel_donor = smf.ols(\"amount ~ treatment\", data=df_donors).fit()\n\n# Output\nprint(\"====Output From the Code Block====\")\nprint(f\"\\nT-test p-value (donors only): {p_val:.4f}\")\nprint(f\"Regression coefficient (treatment effect): {model_donor.params['treatment']:.4f}\")\nprint(f\"Regression p-value: {model_donor.pvalues['treatment']:.4f}\")\n\n\n====Output From the Code Block====\n\nT-test p-value (donors only): 0.5590\nRegression coefficient (treatment effect): -1.6684\nRegression p-value: 0.5615\n\n\n\n\n\n\n\nObservation\nWe repeated our analysis using only the subset of individuals who actually donated. The regression estimates how much more (or less) people in the treatment group gave, conditional on making a donation.\nThe coefficient on treatment is –1.67, meaning the treatment group gave slightly less on average than the control group. However, this difference is not statistically significant (p = 0.5615), so we cannot conclude that there is a meaningful effect.\nWhat does this mean? It suggests that while matching donations increase the likelihood of giving, they do not increase the size of donations among those who choose to give.\nBecause this regression conditions on a post-treatment variable (gave == 1), it does not have a causal interpretation. Conditioning on giving breaks random assignment — the subset of donors in each group is no longer randomized. This analysis is descriptive, not causal. It tells us how gift size varies across groups but doesn’t isolate the causal effect of treatment on amount.\nWe visualize donation amounts separately for treatment and control groups, including only those who made a donation. A red line marks the sample average in each group.\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load and filter data\ndf = pd.read_csv(\"karlan_list_2007.csv\")\ndf = df[df['gave'] == 1]\ndf['treatment'] = df['treatment'].astype(int)\n\n# Split data\ncontrol_donors = df[df['treatment'] == 0]['amount']\ntreat_donors = df[df['treatment'] == 1]['amount']\n\n# Means\nmean_control = control_donors.mean()\nmean_treat = treat_donors.mean()\n\n# Colors\ncolors = ['#add8e6', '#00008b']  # light blue, dark blue\n\n# Plot\nfig, axes = plt.subplots(2, 1, figsize=(8,4), sharex=False)\n\n# Control group plot\naxes[0].hist(control_donors, bins=30, color=colors[0], edgecolor='black')\naxes[0].axvline(mean_control, color='red', linestyle='--', label=f'Mean: ${mean_control:.2f}')\naxes[0].set_title('Control Group (Donors Only)')\naxes[0].set_xlabel('Donation Amount ($)')\naxes[0].set_ylabel('Number of Donors')\naxes[0].legend()\naxes[0].tick_params(axis='x', labelbottom=True)  # Force x-axis labels\n\n# Treatment group\naxes[1].hist(treat_donors, bins=30, color=colors[1], edgecolor='black')\naxes[1].axvline(mean_treat, color='red', linestyle='--', label=f'Mean: ${mean_treat:.2f}')\naxes[1].set_title('Treatment Group (Donors Only)')\naxes[1].set_xlabel('Donation Amount ($)')\naxes[1].set_ylabel('Number of Donors')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nObservation\nThe histograms display the distribution of donation amounts among donors in each group. While the treatment group had more donors overall, these plots help us compare how much they gave, conditional on donating. The red dashed line marks the average donation in each group. Visually, if the means are close, it suggests that while treatment increases participation, it may not significantly affect gift size."
  },
  {
    "objectID": "hw1_questions (2).html",
    "href": "hw1_questions (2).html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions (2).html#introduction",
    "href": "hw1_questions (2).html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions (2).html#data",
    "href": "hw1_questions (2).html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "hw1_questions (2).html#experimental-results",
    "href": "hw1_questions (2).html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "hw1_questions (2).html#simulation-experiment",
    "href": "hw1_questions (2).html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  },
  {
    "objectID": "hw1_questions.html#conditional-analysis-donation-amount-among-donors-only",
    "href": "hw1_questions.html#conditional-analysis-donation-amount-among-donors-only",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Conditional Analysis: Donation Amount Among Donors Only",
    "text": "Conditional Analysis: Donation Amount Among Donors Only\nWe now restrict the data to people who donated (gave == 1) and run a regression to estimate whether the treatment group gave more, conditional on giving.\n\n\nCode\nimport pandas as pd\nfrom scipy.stats import ttest_ind\nimport statsmodels.formula.api as smf\n\n# Load data\ndf = pd.read_csv(\"karlan_list_2007.csv\")\ndf['treatment'] = df['treatment'].astype(int)\n\n# Filter to donors only\ndf_donors = df[df['gave'] == 1].copy()\n\n# T-test: donation amount among donors\ntreat_amt = df_donors[df_donors['treatment'] == 1]['amount']\ncontrol_amt = df_donors[df_donors['treatment'] == 0]['amount']\nt_stat, p_val = ttest_ind(treat_amt, control_amt, equal_var=False)\n\n# Regression: amount ~ treatment (among donors only)\nmodel_donor = smf.ols(\"amount ~ treatment\", data=df_donors).fit()\n\n# Output\nprint(f\"T-test p-value (donors only): {p_val:.4f}\")\nprint(f\"Regression coefficient (treatment effect): {model_donor.params['treatment']:.4f}\")\nprint(f\"Regression p-value: {model_donor.pvalues['treatment']:.4f}\")\n\n\nT-test p-value (donors only): 0.5590\nRegression coefficient (treatment effect): -1.6684\nRegression p-value: 0.5615\n\n\n\nObservation\nWe repeated our analysis using only the subset of individuals who actually donated. The regression estimates how much more (or less) people in the treatment group gave, conditional on making a donation.\nThe coefficient on treatment is –1.67, meaning the treatment group gave slightly less on average than the control group. However, this difference is not statistically significant (p = 0.5615), so we cannot conclude that there is a meaningful effect.\nWhat does this mean? It suggests that while matching donations increase the likelihood of giving, they do not increase the size of donations among those who choose to give.\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot.\nWe visualize donation amounts separately for treatment and control groups, including only those who made a donation. A red line marks the sample average in each group.\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load and filter data\ndf = pd.read_csv(\"karlan_list_2007.csv\")\ndf = df[df['gave'] == 1]\ndf['treatment'] = df['treatment'].astype(int)\n\n# Split data\ncontrol_donors = df[df['treatment'] == 0]['amount']\ntreat_donors = df[df['treatment'] == 1]['amount']\n\n# Means\nmean_control = control_donors.mean()\nmean_treat = treat_donors.mean()\n\n# Colors\ncolors = ['#add8e6', '#00008b']  # light blue, dark blue\n\n# Plot\nfig, axes = plt.subplots(2, 1, figsize=(10, 8), sharex=False)\n\n# Control group plot\naxes[0].hist(control_donors, bins=30, color=colors[0], edgecolor='black')\naxes[0].axvline(mean_control, color='red', linestyle='--', label=f'Mean: ${mean_control:.2f}')\naxes[0].set_title('Control Group (Donors Only)')\naxes[0].set_xlabel('Donation Amount ($)')\naxes[0].set_ylabel('Number of Donors')\naxes[0].legend()\naxes[0].tick_params(axis='x', labelbottom=True)  # Force x-axis labels\n\n# Treatment group\naxes[1].hist(treat_donors, bins=30, color=colors[1], edgecolor='black')\naxes[1].axvline(mean_treat, color='red', linestyle='--', label=f'Mean: ${mean_treat:.2f}')\naxes[1].set_title('Treatment Group (Donors Only)')\naxes[1].set_xlabel('Donation Amount ($)')\naxes[1].set_ylabel('Number of Donors')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "hw1_questions.html#donation-amounts-among-donors-treatment-vs.-control",
    "href": "hw1_questions.html#donation-amounts-among-donors-treatment-vs.-control",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Donation Amounts Among Donors: Treatment vs. Control",
    "text": "Donation Amounts Among Donors: Treatment vs. Control"
  },
  {
    "objectID": "hw1_questions.html#donation-amounts-among-donors-vertical-layout",
    "href": "hw1_questions.html#donation-amounts-among-donors-vertical-layout",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Donation Amounts Among Donors (Vertical Layout)",
    "text": "Donation Amounts Among Donors (Vertical Layout)\n\n\nCode\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Load and filter data\ndf = pd.read_csv(\"karlan_list_2007.csv\")\ndf = df[df['gave'] == 1]\ndf['treatment'] = df['treatment'].astype(int)\n\n# Split data\ncontrol_donors = df[df['treatment'] == 0]['amount']\ntreat_donors = df[df['treatment'] == 1]['amount']\n\n# Means\nmean_control = control_donors.mean()\nmean_treat = treat_donors.mean()\n\n# Colors\ncolors = ['#add8e6', '#00008b']  # light blue, dark blue\n\n# Plot vertically (2 rows, 1 column)\nfig, axes = plt.subplots(2, 1, figsize=(8, 8), sharex=True)\n\n# Control group\naxes[0].hist(control_donors, bins=30, color=colors[0], edgecolor='black')\naxes[0].axvline(mean_control, color='red', linestyle='--', label=f'Mean: ${mean_control:.2f}')\naxes[0].set_title('Control Group (Donors Only)')\naxes[0].set_ylabel('Number of Donors')\naxes[0].legend()\n\n# Treatment group\naxes[1].hist(treat_donors, bins=30, color=colors[1], edgecolor='black')\naxes[1].axvline(mean_treat, color='red', linestyle='--', label=f'Mean: ${mean_treat:.2f}')\naxes[1].set_title('Treatment Group (Donors Only)')\naxes[1].set_xlabel('Donation Amount ($)')\naxes[1].set_ylabel('Number of Donors')\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "hw1_questions.html#law-of-large-numbers-simulation-slide-43-style",
    "href": "hw1_questions.html#law-of-large-numbers-simulation-slide-43-style",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Law of Large Numbers Simulation: Slide 43 Style",
    "text": "Law of Large Numbers Simulation: Slide 43 Style\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\nnp.random.seed(42)\n\n# Simulate 100,000 observations for each group\ncontrol_draws = np.random.binomial(n=1, p=0.018, size=100000)\ntreatment_draws = np.random.binomial(n=1, p=0.022, size=100000)\n\n# Take 10,000 pairwise differences\ndiffs = treatment_draws[:10000] - control_draws[:10000]\n\n# Compute cumulative average\ncumulative_avg = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)\n\n# Plot\nplt.figure(figsize=(10, 5))\nplt.plot(cumulative_avg, label=\"Cumulative Avg of Differences\", color=\"blue\")\nplt.axhline(0.004, color=\"red\", linestyle=\"--\", label=\"True Difference (0.004)\")\nplt.title(\"Law of Large Numbers: Cumulative Average of Sample Differences\")\nplt.xlabel(\"Number of Observations\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "hw1_questions.html#central-limit-theorem-simulation-4-sample-sizes",
    "href": "hw1_questions.html#central-limit-theorem-simulation-4-sample-sizes",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Central Limit Theorem Simulation: 4 Sample Sizes",
    "text": "Central Limit Theorem Simulation: 4 Sample Sizes\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Set seed\nnp.random.seed(42)\n\n# True probabilities\np_control = 0.018\np_treat = 0.022\ntrue_diff = p_treat - p_control\n\n# Sample sizes to simulate\nsample_sizes = [50, 200, 500, 1000]\nnum_simulations = 1000\n\n# Set up 2x2 subplot grid\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n# Flatten axes array for easier iteration\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    # Store average differences for each simulation\n    avg_diffs = []\n\n    for _ in range(num_simulations):\n        control = np.random.binomial(1, p_control, n)\n        treatment = np.random.binomial(1, p_treat, n)\n        avg_diffs.append(np.mean(treatment) - np.mean(control))\n\n    # Plot histogram\n    axes[i].hist(avg_diffs, bins=30, color='skyblue', edgecolor='black')\n    axes[i].axvline(0, color='black', linestyle='--', label='Zero')\n    axes[i].axvline(true_diff, color='red', linestyle='--', label='True Diff = 0.004')\n    axes[i].set_title(f\"Sample Size = {n}\")\n    axes[i].set_xlabel(\"Average Treatment Effect\")\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "hw2_questions.html",
    "href": "hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", bins=20, multiple=\"dodge\")\nplt.title(\"Number of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.groupby(\"iscustomer\")[\"patents\"].mean()\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\n\n\n\n\nFirms that are customers of Blueprinty tend to have more patents on average (≈ 4.13) than non-customers (≈ 3.47). The histogram shows that customer firms are more concentrated at higher patent counts, suggesting a potential positive relationship between using Blueprinty and patent success. However, this pattern may also be influenced by other factors, such as region or firm age, which need to be explored further.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npalette = {0: \"#4C72B0\", 1: \"#DD8452\"}\nplt.figure(figsize=(8, 6))\nsns.countplot(data=df, x=\"region\", hue=\"iscustomer\", palette=palette)\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Map customer status labels\ndf[\"Customer Status\"] = df[\"iscustomer\"].map({0: \"Non-Customer\", 1: \"Customer\"})\n\n# Define matching palette\npalette = {\"Non-Customer\": \"#4C72B0\", \"Customer\": \"#DD8452\"}\n\n# Plot\nplt.figure(figsize=(8, 6))\nsns.boxplot(data=df, x=\"Customer Status\", y=\"age\", hue=\"Customer Status\", palette=palette, legend=False)\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Is Customer\")\nplt.ylabel(\"Firm Age (Years)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.groupby(\"iscustomer\")[\"age\"].mean()\n\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\n\n\n\n\n\n\nThe regional distribution of firms is not uniform between customers and non-customers. Some regions (e.g., Northeast) have a higher concentration of Blueprinty customers. This suggests that region may confound the relationship between software usage and patent outcomes.\nRegarding age, customers are slightly older on average (~26.9 years) than non-customers (~26.1 years), though the difference is modest. It is still important to consider firm age in the analysis to avoid biased conclusions.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\nProbability Mass Function\nThe probability mass function for a single observation from a Poisson distribution is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nLikelihood Function\nAssuming the observations are independent, the likelihood function for the entire dataset is:\n\\[\nL(\\lambda; Y_1, \\ldots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThis can be rewritten as:\n\\[\nL(\\lambda) = e^{-n\\lambda} \\cdot \\lambda^{\\sum_{i=1}^{n} Y_i} \\cdot \\prod_{i=1}^{n} \\frac{1}{Y_i!}\n\\]\nLog-Likelihood Function\nTaking the natural logarithm of the likelihood gives:\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log(Y_i!)\n\\]\nThis log-likelihood will be used to estimate ( ) via Maximum Likelihood Estimation (MLE).\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define the Poisson log-likelihood function\ndef poisson_loglikelihood(lambd, Y):\n    if lambd &lt;= 0:\n        return -np.inf  # log-likelihood is undefined for non-positive lambda\n    return np.sum(-lambd + Y * np.log(lambd) - gammaln(Y + 1))\n\n\n\n\n\nWe visualize the Poisson log-likelihood as a function of ( ), where the maximum corresponds to the MLE.\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\n# Example data: simulated Poisson observations\nY_sample = df[\"patents\"].values\n\n# Range of lambda values to plot\nlambdas = np.linspace(0.1, 10, 300)\nlog_likelihoods = [poisson_loglikelihood(l, Y_sample) for l in lambdas]\n\n# Find MLE visually\nlambda_mle = lambdas[np.argmax(log_likelihoods)]\n\n# Plot\nplt.figure(figsize=(8, 6))\nplt.plot(lambdas, log_likelihoods, linewidth=2, color=\"steelblue\")\nplt.axvline(lambda_mle, color=\"darkorange\", linestyle=\"--\", label=f\"MLE ≈ {lambda_mle:.2f}\")\nplt.title(\"Poisson Log-Likelihood vs. Lambda\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Lambda\", fontsize=12)\nplt.ylabel(\"Log-Likelihood\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose ( Y_1, Y_2, , Y_n () ), where the probability mass function is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nStep 1: Log-Likelihood Function\nThe log-likelihood of the entire sample is:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\nWe can simplify this (since ( Y_i! ) does not depend on ( )):\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda + \\text{constant}\n\\]\nStep 2: First Derivative\nTake the derivative with respect to ( ):\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i\n\\]\nStep 3: Set Derivative to Zero\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n\\]\nSolving for ( ):\n\\[\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThe maximum likelihood estimator (MLE) of ( ) is the sample mean:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\nThis result makes intuitive sense: in a Poisson distribution, the mean and variance are both equal to ( ), so the best estimate of ( ) from data is the observed average.\n\n\n\nWe use numerical optimization to find the value of ( ) that maximizes the Poisson log-likelihood.\n\n\n\n\n\n\n\n\nCode\nfrom scipy import optimize\nneg_loglikelihood = lambda lambd: -poisson_loglikelihood(lambd[0], Y_sample)\nresult = optimize.minimize(neg_loglikelihood, x0=[1.0], bounds=[(1e-6, None)])\nlambda_mle = result.x[0]\nlambda_mle\n\n\n3.684666485763343\n\n\n\n\n\n\n\n\nThe maximum likelihood estimate (MLE) of ( ) is approximately equal to the sample mean of the number of patents, which is expected for a Poisson distribution.\n\n\nCode\n# Compare with the sample mean\ndf[\"patents\"].mean()\n\n\n3.6846666666666668\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define Poisson regression log-likelihood function\ndef poisson_regression_loglike(beta, X, Y):\n    Xbeta = X @ beta\n    lambdas = np.exp(Xbeta)\n    return np.sum(-lambdas + Y * Xbeta - gammaln(Y + 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom scipy import optimize\nfrom scipy.special import gammaln\n\n# Load the data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Create age squared\ndf[\"age2\"] = df[\"age\"] ** 2\n\n# Create region dummies (drop one to avoid multicollinearity)\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\n\n# Construct design matrix\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[\"age\"],\n    df[\"age2\"],\n    region_dummies,\n    df[\"iscustomer\"]\n], axis=1)\n\nY = df[\"patents\"].values\nX_matrix = X.values\ndef poisson_loglike(beta, X, Y):\n    beta = np.atleast_1d(np.asarray(beta))\n    Xb = np.dot(X, beta).astype(np.float64)\n    Xb_clipped = np.clip(Xb, a_min=None, a_max=20)  # cap max exponent\n    lam = np.exp(Xb_clipped)\n\n    return np.sum(-lam + Y * Xb - gammaln(Y + 1))\n\ndef neg_loglike(beta, X, Y):\n    return -poisson_loglike(beta, X, Y)\n\n\ninitial_beta = np.zeros(X.shape[1])\nresult = optimize.minimize(neg_loglike, initial_beta, args=(X_matrix, Y), method='BFGS')\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errs = np.sqrt(np.diag(hessian_inv))\nsummary = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": std_errs\n}, index=X.columns)\n\nsummary\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.509954\n0.193038\n\n\nage\n0.148702\n0.014461\n\n\nage2\n-0.002972\n0.000266\n\n\nNortheast\n0.029159\n0.046761\n\n\nNorthwest\n-0.017578\n0.057234\n\n\nSouth\n0.056567\n0.056243\n\n\nSouthwest\n0.050589\n0.049616\n\n\niscustomer\n0.207600\n0.032939\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the accuracy of our manual MLE implementation, we use statsmodels.GLM() to estimate the same Poisson regression model:\n\n\n\n\n\n\n\n\nCode\nimport statsmodels.api as sm\n\n# Drop 'intercept' column and ensure all data is float\nX_glm = X.drop(columns='intercept', errors='ignore').astype(float)\n\n# Add constant for intercept term\nX_glm = sm.add_constant(X_glm)\n\n# Fit GLM model\nglm_model = sm.GLM(Y, X_glm, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary\nglm_results.summary()\n\n### Coefficients and Standard Errors from Poisson Regression\n# Extract coefficient summary\ncoef_table = glm_results.summary2().tables[1][[\"Coef.\", \"Std.Err.\"]]\ncoef_table.rename(columns={\"Coef.\": \"Coefficient\", \"Std.Err.\": \"Std. Error\"}, inplace=True)\n\n# Display table\ncoef_table\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nconst\n-0.508920\n0.183179\n\n\nage\n0.148619\n0.013869\n\n\nage2\n-0.002970\n0.000258\n\n\nNortheast\n0.029170\n0.043625\n\n\nNorthwest\n-0.017575\n0.053781\n\n\nSouth\n0.056561\n0.052662\n\n\nSouthwest\n0.050576\n0.047198\n\n\niscustomer\n0.207591\n0.030895\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge has a strong positive effect on patent activity: older firms are more likely to have more patents.\nAge² is negative and significant, suggesting a diminishing return — patent output increases with age but at a decreasing rate.\nBlueprinty’s software has a statistically significant positive coefficient of 0.2076 (p &lt; 0.001). This implies firms using the software are expected to have 23% more patents than comparable non-customers.\nRegional effects (Northeast, Northwest, etc.) are not statistically significant, suggesting location does not materially affect patent outcomes once other factors are controlled for.\n\nEstimate Effect of Blueprinty’s Software via Counterfactual Prediction\nWe simulate two scenarios:\n\nX_0: All firms set to non-customer (iscustomer = 0)\nX_1: All firms set to customer (iscustomer = 1)\n\nThen we compare the predicted number of patents for each firm under the two scenarios using the fitted model.\n\n\nCode\n# Make two versions of X_glm:\n# X_0: all firms are non-customers\n# X_1: all firms are customers\nX_0 = X_glm.copy()\nX_1 = X_glm.copy()\n\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected patent counts\ny_pred_0 = glm_results.predict(X_0)\ny_pred_1 = glm_results.predict(X_1)\n\n# Estimate average treatment effect\naverage_effect = np.mean(y_pred_1 - y_pred_0)\n\n\n\n\n\nThe average difference in predicted number of patents between Blueprinty customers and non-customers is:\n\n\nCode\nprint(f\"Estimated average increase in patent count from using Blueprinty: {average_effect:.3f}\")\n\n\nEstimated average increase in patent count from using Blueprinty: 0.793\n\n\nThis quantifies the effect of Blueprinty’s software: firms using it are predicted to file approximately 0.793 more patents over 5 years, on average, than similar firms who don’t use it, controlling for age and region."
  },
  {
    "objectID": "hw2_questions.html#blueprinty-case-study",
    "href": "hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", bins=20, multiple=\"dodge\")\nplt.title(\"Number of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.groupby(\"iscustomer\")[\"patents\"].mean()\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\n\n\n\n\n\nFirms that are customers of Blueprinty tend to have more patents on average (≈ 4.13) than non-customers (≈ 3.47). The histogram shows that customer firms are more concentrated at higher patent counts, suggesting a potential positive relationship between using Blueprinty and patent success. However, this pattern may also be influenced by other factors, such as region or firm age, which need to be explored further.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npalette = {0: \"#4C72B0\", 1: \"#DD8452\"}\nplt.figure(figsize=(8, 6))\nsns.countplot(data=df, x=\"region\", hue=\"iscustomer\", palette=palette)\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Map customer status labels\ndf[\"Customer Status\"] = df[\"iscustomer\"].map({0: \"Non-Customer\", 1: \"Customer\"})\n\n# Define matching palette\npalette = {\"Non-Customer\": \"#4C72B0\", \"Customer\": \"#DD8452\"}\n\n# Plot\nplt.figure(figsize=(8, 6))\nsns.boxplot(data=df, x=\"Customer Status\", y=\"age\", hue=\"Customer Status\", palette=palette, legend=False)\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Is Customer\")\nplt.ylabel(\"Firm Age (Years)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.groupby(\"iscustomer\")[\"age\"].mean()\n\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\n\n\n\n\n\n\nThe regional distribution of firms is not uniform between customers and non-customers. Some regions (e.g., Northeast) have a higher concentration of Blueprinty customers. This suggests that region may confound the relationship between software usage and patent outcomes.\nRegarding age, customers are slightly older on average (~26.9 years) than non-customers (~26.1 years), though the difference is modest. It is still important to consider firm age in the analysis to avoid biased conclusions.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\n\nProbability Mass Function\nThe probability mass function for a single observation from a Poisson distribution is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nLikelihood Function\nAssuming the observations are independent, the likelihood function for the entire dataset is:\n\\[\nL(\\lambda; Y_1, \\ldots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThis can be rewritten as:\n\\[\nL(\\lambda) = e^{-n\\lambda} \\cdot \\lambda^{\\sum_{i=1}^{n} Y_i} \\cdot \\prod_{i=1}^{n} \\frac{1}{Y_i!}\n\\]\nLog-Likelihood Function\nTaking the natural logarithm of the likelihood gives:\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log(Y_i!)\n\\]\nThis log-likelihood will be used to estimate ( ) via Maximum Likelihood Estimation (MLE).\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define the Poisson log-likelihood function\ndef poisson_loglikelihood(lambd, Y):\n    if lambd &lt;= 0:\n        return -np.inf  # log-likelihood is undefined for non-positive lambda\n    return np.sum(-lambd + Y * np.log(lambd) - gammaln(Y + 1))\n\n\n\n\n\nWe visualize the Poisson log-likelihood as a function of ( ), where the maximum corresponds to the MLE.\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\n# Example data: simulated Poisson observations\nY_sample = df[\"patents\"].values\n\n# Range of lambda values to plot\nlambdas = np.linspace(0.1, 10, 300)\nlog_likelihoods = [poisson_loglikelihood(l, Y_sample) for l in lambdas]\n\n# Find MLE visually\nlambda_mle = lambdas[np.argmax(log_likelihoods)]\n\n# Plot\nplt.figure(figsize=(8, 6))\nplt.plot(lambdas, log_likelihoods, linewidth=2, color=\"steelblue\")\nplt.axvline(lambda_mle, color=\"darkorange\", linestyle=\"--\", label=f\"MLE ≈ {lambda_mle:.2f}\")\nplt.title(\"Poisson Log-Likelihood vs. Lambda\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Lambda\", fontsize=12)\nplt.ylabel(\"Log-Likelihood\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose ( Y_1, Y_2, , Y_n () ), where the probability mass function is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nStep 1: Log-Likelihood Function\nThe log-likelihood of the entire sample is:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\nWe can simplify this (since ( Y_i! ) does not depend on ( )):\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda + \\text{constant}\n\\]\nStep 2: First Derivative\nTake the derivative with respect to ( ):\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i\n\\]\nStep 3: Set Derivative to Zero\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n\\]\nSolving for ( ):\n\\[\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThe maximum likelihood estimator (MLE) of ( ) is the sample mean:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\nThis result makes intuitive sense: in a Poisson distribution, the mean and variance are both equal to ( ), so the best estimate of ( ) from data is the observed average.\n\n\n\nWe use numerical optimization to find the value of ( ) that maximizes the Poisson log-likelihood.\n\n\n\n\n\n\n\n\nCode\nfrom scipy import optimize\nneg_loglikelihood = lambda lambd: -poisson_loglikelihood(lambd[0], Y_sample)\nresult = optimize.minimize(neg_loglikelihood, x0=[1.0], bounds=[(1e-6, None)])\nlambda_mle = result.x[0]\nlambda_mle\n\n\n3.684666485763343\n\n\n\n\n\n\n\n\nThe maximum likelihood estimate (MLE) of ( ) is approximately equal to the sample mean of the number of patents, which is expected for a Poisson distribution.\n\n\nCode\n# Compare with the sample mean\ndf[\"patents\"].mean()\n\n\n3.6846666666666668\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define Poisson regression log-likelihood function\ndef poisson_regression_loglike(beta, X, Y):\n    Xbeta = X @ beta\n    lambdas = np.exp(Xbeta)\n    return np.sum(-lambdas + Y * Xbeta - gammaln(Y + 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom scipy import optimize\nfrom scipy.special import gammaln\n\n# Load the data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Create age squared\ndf[\"age2\"] = df[\"age\"] ** 2\n\n# Create region dummies (drop one to avoid multicollinearity)\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\n\n# Construct design matrix\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[\"age\"],\n    df[\"age2\"],\n    region_dummies,\n    df[\"iscustomer\"]\n], axis=1)\n\nY = df[\"patents\"].values\nX_matrix = X.values\ndef poisson_loglike(beta, X, Y):\n    beta = np.atleast_1d(np.asarray(beta))\n    Xb = np.dot(X, beta).astype(np.float64)\n    Xb_clipped = np.clip(Xb, a_min=None, a_max=20)  # cap max exponent\n    lam = np.exp(Xb_clipped)\n\n    return np.sum(-lam + Y * Xb - gammaln(Y + 1))\n\ndef neg_loglike(beta, X, Y):\n    return -poisson_loglike(beta, X, Y)\n\n\ninitial_beta = np.zeros(X.shape[1])\nresult = optimize.minimize(neg_loglike, initial_beta, args=(X_matrix, Y), method='BFGS')\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errs = np.sqrt(np.diag(hessian_inv))\nsummary = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": std_errs\n}, index=X.columns)\n\nsummary\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.509954\n0.193038\n\n\nage\n0.148702\n0.014461\n\n\nage2\n-0.002972\n0.000266\n\n\nNortheast\n0.029159\n0.046761\n\n\nNorthwest\n-0.017578\n0.057234\n\n\nSouth\n0.056567\n0.056243\n\n\nSouthwest\n0.050589\n0.049616\n\n\niscustomer\n0.207600\n0.032939\n\n\n\n\n\n\n\n\n\n\n\n\n\nTo confirm the accuracy of our manual MLE implementation, we use statsmodels.GLM() to estimate the same Poisson regression model:\n\n\n\n\n\n\n\n\nCode\nimport statsmodels.api as sm\n\n# Drop 'intercept' column and ensure all data is float\nX_glm = X.drop(columns='intercept', errors='ignore').astype(float)\n\n# Add constant for intercept term\nX_glm = sm.add_constant(X_glm)\n\n# Fit GLM model\nglm_model = sm.GLM(Y, X_glm, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary\nglm_results.summary()\n\n### Coefficients and Standard Errors from Poisson Regression\n# Extract coefficient summary\ncoef_table = glm_results.summary2().tables[1][[\"Coef.\", \"Std.Err.\"]]\ncoef_table.rename(columns={\"Coef.\": \"Coefficient\", \"Std.Err.\": \"Std. Error\"}, inplace=True)\n\n# Display table\ncoef_table\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nconst\n-0.508920\n0.183179\n\n\nage\n0.148619\n0.013869\n\n\nage2\n-0.002970\n0.000258\n\n\nNortheast\n0.029170\n0.043625\n\n\nNorthwest\n-0.017575\n0.053781\n\n\nSouth\n0.056561\n0.052662\n\n\nSouthwest\n0.050576\n0.047198\n\n\niscustomer\n0.207591\n0.030895\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAge has a strong positive effect on patent activity: older firms are more likely to have more patents.\nAge² is negative and significant, suggesting a diminishing return — patent output increases with age but at a decreasing rate.\nBlueprinty’s software has a statistically significant positive coefficient of 0.2076 (p &lt; 0.001). This implies firms using the software are expected to have 23% more patents than comparable non-customers.\nRegional effects (Northeast, Northwest, etc.) are not statistically significant, suggesting location does not materially affect patent outcomes once other factors are controlled for.\n\nEstimate Effect of Blueprinty’s Software via Counterfactual Prediction\nWe simulate two scenarios:\n\nX_0: All firms set to non-customer (iscustomer = 0)\nX_1: All firms set to customer (iscustomer = 1)\n\nThen we compare the predicted number of patents for each firm under the two scenarios using the fitted model.\n\n\nCode\n# Make two versions of X_glm:\n# X_0: all firms are non-customers\n# X_1: all firms are customers\nX_0 = X_glm.copy()\nX_1 = X_glm.copy()\n\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected patent counts\ny_pred_0 = glm_results.predict(X_0)\ny_pred_1 = glm_results.predict(X_1)\n\n# Estimate average treatment effect\naverage_effect = np.mean(y_pred_1 - y_pred_0)\n\n\n\n\n\nThe average difference in predicted number of patents between Blueprinty customers and non-customers is:\n\n\nCode\nprint(f\"Estimated average increase in patent count from using Blueprinty: {average_effect:.3f}\")\n\n\nEstimated average increase in patent count from using Blueprinty: 0.793\n\n\nThis quantifies the effect of Blueprinty’s software: firms using it are predicted to file approximately 0.793 more patents over 5 years, on average, than similar firms who don’t use it, controlling for age and region."
  },
  {
    "objectID": "hw2_questions.html#airbnb-case-study",
    "href": "hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nData Cleaning\nWe begin by dropping listings with missing values in relevant variables, then perform basic EDA on the cleaned dataset.\n\n\nDrop Rows with Missing Data\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv(\"airbnb.csv\")\nrelevant_cols = [\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\", \"instant_bookable\"\n]\ndf_clean = df[relevant_cols].dropna()\n\n\n\n\nExploratory Data Analysis (EDA)\n\n\nDistribution of Number of Reviews\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 6))\nsns.histplot(\n    df_clean[\"number_of_reviews\"],\n    bins=50,\n    kde=False,\n    color=\"#1F78B4\",\n\n)\n\nplt.title(\"Distribution of Number of Reviews\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Number of Reviews\", fontsize=12)\nplt.ylabel(\"Count of Listings\", fontsize=12)\nplt.xlim(0, 100)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.5)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAverage Number of Reviews by Room Type\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n# Data\navg_reviews = df_clean.groupby(\"room_type\")[\"number_of_reviews\"].mean().reset_index()\n\n# Better-looking custom blue palette\ncustom_blue_palette = [\"#A6CEE3\", \"#1F78B4\", \"#08519C\"]\n\n# Plot with warning suppression\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\", category=FutureWarning)\n\n    plt.figure(figsize=(8, 6))\n    sns.barplot(\n        data=avg_reviews,\n        x=\"room_type\",\n        y=\"number_of_reviews\",\n        palette=custom_blue_palette,\n    )\n\nplt.title(\"Average Number of Reviews by Room Type\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Room Type\", fontsize=12)\nplt.ylabel(\"Average Number of Reviews\", fontsize=12)\nplt.xticks(rotation=0)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAverage Number of Reviews by Instant Bookability\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\navg_reviews_by_bookable = df_clean.groupby(\"instant_bookable\")[\"number_of_reviews\"].mean().reset_index()\navg_reviews_by_bookable[\"instant_bookable\"] = avg_reviews_by_bookable[\"instant_bookable\"].map({\"f\": \"No\", \"t\": \"Yes\"})\n\nblue_palette = {\"No\": \"#6baed6\", \"Yes\": \"#2171b5\"}\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n\n    plt.figure(figsize=(8, 6))\n    sns.barplot(\n        data=avg_reviews_by_bookable,\n        x=\"instant_bookable\",\n        y=\"number_of_reviews\",\n        hue=\"instant_bookable\", \n        palette=blue_palette,\n        legend=False,\n    )\nplt.title(\"Average Number of Reviews by Instant Bookability\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Instant Bookable\", fontsize=12)\nplt.ylabel(\"Average Number of Reviews\", fontsize=12)\nplt.ylim(0, avg_reviews_by_bookable[\"number_of_reviews\"].max() * 1.1)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCorrelation with Numeric Predictors\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nnumeric_vars = [\n    \"number_of_reviews\", \"bathrooms\", \"bedrooms\", \"price\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"\n]\ncorrelation_matrix = df_clean[numeric_vars].corr()\nplt.figure(figsize=(10, 8))\nsns.heatmap(\n    correlation_matrix,\n    annot=True,\n    fmt=\".2f\",\n    cmap=\"Blues\",             \n    vmin=0, vmax=1,          \n    square=True,\n    linewidths=0.75,\n    linecolor=\"white\",\n    annot_kws={\"fontsize\": 10, \"weight\": \"bold\"}\n)\nplt.title(\"Correlation Matrix of Numeric Variables\", fontsize=14, weight=\"bold\")\nplt.xticks(rotation=45, ha=\"right\", fontsize=10)\nplt.yticks(rotation=0, fontsize=10)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Model Using statsmodels.GLM()\nWe model the number of reviews (as a proxy for bookings) using a Poisson regression with the following predictors: - room_type (categorical) - instant_bookable (binary) - price, days, bathrooms, bedrooms - Review scores: cleanliness, location, value\n\n\nCode\nimport statsmodels.api as sm\n\nroom_dummies = pd.get_dummies(df_clean[\"room_type\"], drop_first=True)\ndf_clean[\"instant_bookable\"] = df_clean[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\n# Create design matrix\nX = pd.concat([\n    df_clean[[\"price\", \"days\", \"bathrooms\", \"bedrooms\",\n              \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n              \"instant_bookable\"]],\n    room_dummies\n], axis=1)\n\nX = sm.add_constant(X)\nX = X.astype(float)  \n\nY = df_clean[\"number_of_reviews\"]\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\nsummary_df = poisson_results.summary2().tables[1]\n\nsummary_df = summary_df.rename(columns={\n    \"Coef.\": \"Coefficient\",\n    \"Std.Err.\": \"Std. Error\",\n    \"P&gt;|z|\": \"P-Value\"\n})\n\nsignificant_results = summary_df[summary_df[\"P-Value\"] &lt; 0.05][[\"Coefficient\", \"Std. Error\", \"P-Value\"]]\n\nsignificant_results = significant_results.round(4)\n\nsignificant_results\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nP-Value\n\n\n\n\nconst\n3.4980\n0.0161\n0.0000\n\n\nprice\n-0.0000\n0.0000\n0.0315\n\n\ndays\n0.0001\n0.0000\n0.0000\n\n\nbathrooms\n-0.1177\n0.0037\n0.0000\n\n\nbedrooms\n0.0741\n0.0020\n0.0000\n\n\nreview_scores_cleanliness\n0.1131\n0.0015\n0.0000\n\n\nreview_scores_location\n-0.0769\n0.0016\n0.0000\n\n\nreview_scores_value\n-0.0911\n0.0018\n0.0000\n\n\ninstant_bookable\n0.3459\n0.0029\n0.0000\n\n\nPrivate room\n-0.0105\n0.0027\n0.0001\n\n\nShared room\n-0.2463\n0.0086\n0.0000\n\n\n\n\n\n\n\n\n\nInterpretation\n\nIntercept (3.4980)\nThe baseline log-expected number of reviews for a listing when all other variables are zero (serves as a reference point).\nPrice (-0.0000)\nAs price increases, the expected number of reviews decreases slightly. This effect is small but statistically significant, indicating higher-priced listings may deter some bookings.\nDays Active (+0.0001)\nListings that have been active longer tend to accumulate more reviews. This reflects more exposure over time.\nBathrooms (-0.1177)\nSurprisingly, listings with more bathrooms tend to receive fewer reviews. This might reflect that larger or luxury properties are booked less frequently.\nBedrooms (+0.0741)\nListings with more bedrooms attract more bookings, likely due to their ability to accommodate larger groups.\nCleanliness Score (+0.1131)\nClean listings receive more reviews, reinforcing the importance of cleanliness to guests.\nLocation Score (-0.0769)\nA negative association with reviews, possibly due to limited variation in location ratings or multicollinearity with other variables.\nValue Score (-0.0911)\nHigher value scores are associated with fewer reviews, which may reflect that guests leave high value ratings in less competitive or less popular markets.\nInstant Bookable (+0.3459)\nListings that support instant booking are expected to receive approximately 41% more reviews than those that do not:\n[ (0.3459) ]\nPrivate Room (-0.0105)\nPrivate rooms receive slightly fewer reviews than entire homes, likely due to lower demand or guest preferences for full space.\nShared Room (-0.2463)\nShared rooms receive significantly fewer reviews — about 22% fewer than entire homes:\n[ (-0.2463) ]"
  },
  {
    "objectID": "hw2_questions.html#compare-histograms-and-means-of-number-of-patents-by-customer-status",
    "href": "hw2_questions.html#compare-histograms-and-means-of-number-of-patents-by-customer-status",
    "title": "Poisson Regression Examples",
    "section": "Compare Histograms and Means of Number of Patents by Customer Status",
    "text": "Compare Histograms and Means of Number of Patents by Customer Status\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nplt.figure(figsize=(8, 6))\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", bins=20, multiple=\"dodge\")\nplt.title(\"Number of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\ndf.groupby(\"iscustomer\")[\"patents\"].mean()\n\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\n\nObservation\nFirms that are customers of Blueprinty tend to have more patents on average (≈ 4.13) than non-customers (≈ 3.47). The histogram shows that customer firms are more concentrated at higher patent counts, suggesting a potential positive relationship between using Blueprinty and patent success. However, this pattern may also be influenced by other factors, such as region or firm age, which need to be explored further.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers."
  },
  {
    "objectID": "hw2_questions.html#compare-regions-and-ages-by-customer-status",
    "href": "hw2_questions.html#compare-regions-and-ages-by-customer-status",
    "title": "Poisson Regression Examples",
    "section": "Compare Regions and Ages by Customer Status",
    "text": "Compare Regions and Ages by Customer Status\n\nRegion Distribution by Customer Status\n\n\n\n\n\n\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\npalette = {0: \"#4C72B0\", 1: \"#DD8452\"}\nplt.figure(figsize=(8, 6))\nsns.countplot(data=df, x=\"region\", hue=\"iscustomer\", palette=palette)\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Number of Firms\")\nplt.legend(title=\"Is Customer\", labels=[\"Non-Customer\", \"Customer\"])\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirm Age by Customer Status\n\n\n\n\n\n\n\n\nCode\n# Map customer status labels\ndf[\"Customer Status\"] = df[\"iscustomer\"].map({0: \"Non-Customer\", 1: \"Customer\"})\n\n# Define matching palette\npalette = {\"Non-Customer\": \"#4C72B0\", \"Customer\": \"#DD8452\"}\n\n# Plot\nplt.figure(figsize=(8, 6))\nsns.boxplot(data=df, x=\"Customer Status\", y=\"age\", hue=\"Customer Status\", palette=palette, legend=False)\nplt.title(\"Firm Age by Customer Status\")\nplt.xlabel(\"Is Customer\")\nplt.ylabel(\"Firm Age (Years)\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMean of ages by Customer Status\n\n\n\n\n\n\n\n\nCode\ndf.groupby(\"iscustomer\")[\"age\"].mean()\n\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\n\n\n\n\n\nObservation\nThe regional distribution of firms is not uniform between customers and non-customers. Some regions (e.g., Northeast) have a higher concentration of Blueprinty customers. This suggests that region may confound the relationship between software usage and patent outcomes.\nRegarding age, customers are slightly older on average (~26.9 years) than non-customers (~26.1 years), though the difference is modest. It is still important to consider firm age in the analysis to avoid biased conclusions.\n\n\nEstimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\n\n\nLikelihood for Poisson Distribution\nProbability Mass Function\nThe probability mass function for a single observation from a Poisson distribution is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nLikelihood Function\nAssuming the observations are independent, the likelihood function for the entire dataset is:\n\\[\nL(\\lambda; Y_1, \\ldots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThis can be rewritten as:\n\\[\nL(\\lambda) = e^{-n\\lambda} \\cdot \\lambda^{\\sum_{i=1}^{n} Y_i} \\cdot \\prod_{i=1}^{n} \\frac{1}{Y_i!}\n\\]\nLog-Likelihood Function\nTaking the natural logarithm of the likelihood gives:\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log(Y_i!)\n\\]\nThis log-likelihood will be used to estimate ( ) via Maximum Likelihood Estimation (MLE).\n\n\nPoisson Log Likelihood Function\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define the Poisson log-likelihood function\ndef poisson_loglikelihood(lambd, Y):\n    if lambd &lt;= 0:\n        return -np.inf  # log-likelihood is undefined for non-positive lambda\n    return np.sum(-lambd + Y * np.log(lambd) - gammaln(Y + 1))\n\n\n\n\n\nWe visualize the Poisson log-likelihood as a function of ( ), where the maximum corresponds to the MLE.\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\n# Example data: simulated Poisson observations\nY_sample = df[\"patents\"].values\n\n# Range of lambda values to plot\nlambdas = np.linspace(0.1, 10, 300)\nlog_likelihoods = [poisson_loglikelihood(l, Y_sample) for l in lambdas]\n\n# Find MLE visually\nlambda_mle = lambdas[np.argmax(log_likelihoods)]\n\n# Plot\nplt.figure(figsize=(8, 6))\nplt.plot(lambdas, log_likelihoods, linewidth=2, color=\"steelblue\")\nplt.axvline(lambda_mle, color=\"darkorange\", linestyle=\"--\", label=f\"MLE ≈ {lambda_mle:.2f}\")\nplt.title(\"Poisson Log-Likelihood vs. Lambda\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Lambda\", fontsize=12)\nplt.ylabel(\"Log-Likelihood\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDerivation of the MLE for Poisson Distribution\nSuppose ( Y_1, Y_2, , Y_n () ), where the probability mass function is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nStep 1: Log-Likelihood Function\nThe log-likelihood of the entire sample is:\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\nWe can simplify this (since ( Y_i! ) does not depend on ( )):\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda + \\text{constant}\n\\]\nStep 2: First Derivative\nTake the derivative with respect to ( ):\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i\n\\]\nStep 3: Set Derivative to Zero\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n\\]\nSolving for ( ):\n\\[\n\\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\nThe maximum likelihood estimator (MLE) of ( ) is the sample mean:\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\nThis result makes intuitive sense: in a Poisson distribution, the mean and variance are both equal to ( ), so the best estimate of ( ) from data is the observed average.\n\n\nMaximum Likelihood Estimation using scipy.optimize\nWe use numerical optimization to find the value of ( ) that maximizes the Poisson log-likelihood.\n\n\n\n\n\n\n\n\nCode\nfrom scipy import optimize\nneg_loglikelihood = lambda lambd: -poisson_loglikelihood(lambd[0], Y_sample)\nresult = optimize.minimize(neg_loglikelihood, x0=[1.0], bounds=[(1e-6, None)])\nlambda_mle = result.x[0]\nlambda_mle\n\n\n3.684666485763343\n\n\n\n\n\n\n\nInterpretation\nThe maximum likelihood estimate (MLE) of ( ) is approximately equal to the sample mean of the number of patents, which is expected for a Poisson distribution.\n\n\nCode\n# Compare with the sample mean\ndf[\"patents\"].mean()\n\n\n3.6846666666666668\n\n\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define Poisson regression log-likelihood function\ndef poisson_regression_loglike(beta, X, Y):\n    Xbeta = X @ beta\n    lambdas = np.exp(Xbeta)\n    return np.sum(-lambdas + Y * Xbeta - gammaln(Y + 1))\n\n\n\n\n\n\n\nTo find the MLE vector using scipy.optimize\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom scipy import optimize\nfrom scipy.special import gammaln\n\n# Load the data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Create age squared\ndf[\"age2\"] = df[\"age\"] ** 2\n\n# Create region dummies (drop one to avoid multicollinearity)\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\n\n# Construct design matrix\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[\"age\"],\n    df[\"age2\"],\n    region_dummies,\n    df[\"iscustomer\"]\n], axis=1)\n\nY = df[\"patents\"].values\nX_matrix = X.values\ndef poisson_loglike(beta, X, Y):\n    beta = np.atleast_1d(np.asarray(beta))\n    Xb = np.dot(X, beta).astype(np.float64)\n    Xb_clipped = np.clip(Xb, a_min=None, a_max=20)  # cap max exponent\n    lam = np.exp(Xb_clipped)\n\n    return np.sum(-lam + Y * Xb - gammaln(Y + 1))\n\ndef neg_loglike(beta, X, Y):\n    return -poisson_loglike(beta, X, Y)\n\n\ninitial_beta = np.zeros(X.shape[1])\nresult = optimize.minimize(neg_loglike, initial_beta, args=(X_matrix, Y), method='BFGS')\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errs = np.sqrt(np.diag(hessian_inv))\nsummary = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": std_errs\n}, index=X.columns)\n\nsummary\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.509954\n0.193038\n\n\nage\n0.148702\n0.014461\n\n\nage2\n-0.002972\n0.000266\n\n\nNortheast\n0.029159\n0.046761\n\n\nNorthwest\n-0.017578\n0.057234\n\n\nSouth\n0.056567\n0.056243\n\n\nSouthwest\n0.050589\n0.049616\n\n\niscustomer\n0.207600\n0.032939\n\n\n\n\n\n\n\n\n\n\n\n\nValidate Results Using statsmodels.GLM()\nTo confirm the accuracy of our manual MLE implementation, we use statsmodels.GLM() to estimate the same Poisson regression model:\n\n\n\n\n\n\n\n\nCode\nimport statsmodels.api as sm\n\n# Drop 'intercept' column and ensure all data is float\nX_glm = X.drop(columns='intercept', errors='ignore').astype(float)\n\n# Add constant for intercept term\nX_glm = sm.add_constant(X_glm)\n\n# Fit GLM model\nglm_model = sm.GLM(Y, X_glm, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary\nglm_results.summary()\n\n### Coefficients and Standard Errors from Poisson Regression\n# Extract coefficient summary\ncoef_table = glm_results.summary2().tables[1][[\"Coef.\", \"Std.Err.\"]]\ncoef_table.rename(columns={\"Coef.\": \"Coefficient\", \"Std.Err.\": \"Std. Error\"}, inplace=True)\n\n# Display table\ncoef_table\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nconst\n-0.508920\n0.183179\n\n\nage\n0.148619\n0.013869\n\n\nage2\n-0.002970\n0.000258\n\n\nNortheast\n0.029170\n0.043625\n\n\nNorthwest\n-0.017575\n0.053781\n\n\nSouth\n0.056561\n0.052662\n\n\nSouthwest\n0.050576\n0.047198\n\n\niscustomer\n0.207591\n0.030895\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of Results\n\nAge has a strong positive effect on patent activity: older firms are more likely to have more patents.\nAge² is negative and significant, suggesting a diminishing return — patent output increases with age but at a decreasing rate.\nBlueprinty’s software has a statistically significant positive coefficient of 0.2076 (p &lt; 0.001). This implies firms using the software are expected to have 23% more patents than comparable non-customers.\nRegional effects (Northeast, Northwest, etc.) are not statistically significant, suggesting location does not materially affect patent outcomes once other factors are controlled for.\n\nEstimate Effect of Blueprinty’s Software via ** Counterfactual Prediction**\nWe simulate two scenarios: - X_0: All firms set to non-customer (iscustomer = 0) - X_1: All firms set to customer (iscustomer = 1)\nThen we compare the predicted number of patents for each firm under the two scenarios using the fitted model.\n\n\nCode\n# Make two versions of X_glm:\n# X_0: all firms are non-customers\n# X_1: all firms are customers\nX_0 = X_glm.copy()\nX_1 = X_glm.copy()\n\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected patent counts\ny_pred_0 = glm_results.predict(X_0)\ny_pred_1 = glm_results.predict(X_1)\n\n# Estimate average treatment effect\naverage_effect = np.mean(y_pred_1 - y_pred_0)\n\n\n\n\nInterpretation\nThe average difference in predicted number of patents between Blueprinty customers and non-customers is:\n\n\nCode\nprint(f\"Estimated average increase in patent count from using Blueprinty: {average_effect:.3f}\")\n\n\nEstimated average increase in patent count from using Blueprinty: 0.793\n\n\nThis quantifies the effect of Blueprinty’s software: firms using it are predicted to file approximately 0.793 more patents over 5 years, on average, than similar firms who don’t use it, controlling for age and region."
  },
  {
    "objectID": "hw2_questions.html#maximum-likelihood-estimation-using-scipy.optimize",
    "href": "hw2_questions.html#maximum-likelihood-estimation-using-scipy.optimize",
    "title": "Poisson Regression Examples",
    "section": "Maximum Likelihood Estimation using scipy.optimize",
    "text": "Maximum Likelihood Estimation using scipy.optimize\nWe use numerical optimization to find the value of ( ) that maximizes the Poisson log-likelihood.\n\n\n\n\n\n\n\n\nCode\nfrom scipy import optimize\nneg_loglikelihood = lambda lambd: -poisson_loglikelihood(lambd[0], Y_sample)\nresult = optimize.minimize(neg_loglikelihood, x0=[1.0], bounds=[(1e-6, None)])\nlambda_mle = result.x[0]\nlambda_mle\n\n\n3.684666485763343\n\n\n\n\n\n\nResult\nThe maximum likelihood estimate (MLE) of ( ) is approximately equal to the sample mean of the number of patents, which is expected for a Poisson distribution.\n\n\nCode\n# Compare with the sample mean\ndf[\"patents\"].mean()\n\n\n3.6846666666666668\n\n\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define Poisson regression log-likelihood function\ndef poisson_regression_loglike(beta, X, Y):\n    Xbeta = X @ beta\n    lambdas = np.exp(Xbeta)\n    return np.sum(-lambdas + Y * Xbeta - gammaln(Y + 1))\n\n\n\n\n\n\n\nTo find the MLE vector using sp.optimize()\n\n\n\n\n\n\n\n\nCode\nimport pandas as pd\nimport numpy as np\nfrom scipy import optimize\nfrom scipy.special import gammaln\n\n# Load the data\ndf = pd.read_csv(\"blueprinty.csv\")\n\n# Create age squared\ndf[\"age2\"] = df[\"age\"] ** 2\n\n# Create region dummies (drop one to avoid multicollinearity)\nregion_dummies = pd.get_dummies(df[\"region\"], drop_first=True)\n\n# Construct design matrix\nX = pd.concat([\n    pd.Series(1, index=df.index, name=\"intercept\"),\n    df[\"age\"],\n    df[\"age2\"],\n    region_dummies,\n    df[\"iscustomer\"]\n], axis=1)\n\nY = df[\"patents\"].values\nX_matrix = X.values\ndef poisson_loglike(beta, X, Y):\n    beta = np.atleast_1d(np.asarray(beta))\n    Xb = np.dot(X, beta).astype(np.float64)\n    Xb_clipped = np.clip(Xb, a_min=None, a_max=20)  # cap max exponent\n    lam = np.exp(Xb_clipped)\n\n    return np.sum(-lam + Y * Xb - gammaln(Y + 1))\n\ndef neg_loglike(beta, X, Y):\n    return -poisson_loglike(beta, X, Y)\n\n\ninitial_beta = np.zeros(X.shape[1])\nresult = optimize.minimize(neg_loglike, initial_beta, args=(X_matrix, Y), method='BFGS')\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errs = np.sqrt(np.diag(hessian_inv))\nsummary = pd.DataFrame({\n    \"Coefficient\": beta_hat,\n    \"Std. Error\": std_errs\n}, index=X.columns)\n\nsummary\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nintercept\n-0.509954\n0.193038\n\n\nage\n0.148702\n0.014461\n\n\nage2\n-0.002972\n0.000266\n\n\nNortheast\n0.029159\n0.046761\n\n\nNorthwest\n-0.017578\n0.057234\n\n\nSouth\n0.056567\n0.056243\n\n\nSouthwest\n0.050589\n0.049616\n\n\niscustomer\n0.207600\n0.032939"
  },
  {
    "objectID": "hw2_questions.html#likelihood-for-poisson-distribution",
    "href": "hw2_questions.html#likelihood-for-poisson-distribution",
    "title": "Poisson Regression Examples",
    "section": "Likelihood for Poisson Distribution",
    "text": "Likelihood for Poisson Distribution"
  },
  {
    "objectID": "hw2_questions.html#likelihood-for-poisson-distribution-1",
    "href": "hw2_questions.html#likelihood-for-poisson-distribution-1",
    "title": "Poisson Regression Examples",
    "section": "Likelihood for Poisson Distribution",
    "text": "Likelihood for Poisson Distribution\nSuppose we observe ( Y_1, Y_2, , Y_n ) as independent and identically distributed random variables, each following a Poisson distribution with rate parameter ( ):\n[ Y_i () ]\n\nProbability Mass Function\nThe probability mass function for a single observation is:\n[ f(Y_i ) = ]\n\n\nLikelihood Function\nAssuming independence, the likelihood function for the full sample is:\n[ L(; Y_1, , Y_n) = _{i=1}^{n} ]\nThis can be simplified as:\n[ L() = e^{-n} {{i=1}^{n} Y_i} {i=1}{n} ]\n\n\nLog-Likelihood Function\nTaking the natural logarithm of the likelihood, we get the log-likelihood:\n[ L() = -n+ ( {i=1}^{n} Y_i ) - {i=1}^{n} (Y_i!) ]\nThis log-likelihood will be used to estimate ( ) via Maximum Likelihood Estimation (MLE).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\n\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define the Poisson log-likelihood function\ndef poisson_loglikelihood(lambd, Y):\n    if lambd &lt;= 0:\n        return -np.inf  # log-likelihood is undefined for non-positive lambda\n    return np.sum(-lambd + Y * np.log(lambd) - gammaln(Y + 1))\n\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\nimport matplotlib.pyplot as plt\n\nY = df[\"patents\"].values\n\n# Lambda range\nlambdas = np.linspace(0.1, 10, 200)\nlog_likelihoods = [poisson_loglikelihood(lam, Y) for lam in lambdas]\n\n# Plot\nplt.figure(figsize=(8, 6))\nplt.plot(lambdas, log_likelihoods)\nplt.title(\"Poisson Log-Likelihood vs. Lambda\")\nplt.xlabel(\"Lambda\")\nplt.ylabel(\"Log-Likelihood\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\n\n\nDerivation of the MLE for Poisson Distribution\nSuppose ( Y_1, Y_2, , Y_n () ), where the probability mass function is:\n[ f(Y_i | ) = ]\n\nStep 1: Log-Likelihood Function\nThe log-likelihood of the entire sample is:\n[ L() = _{i=1}^{n} ( -+ Y_i - Y_i! ) ]\nWe can simplify this (since ( Y_i! ) does not depend on ( )):\n[ L() = -n+ ( _{i=1}^{n} Y_i ) + ]\n\n\nStep 2: First Derivative\nTake the derivative with respect to ( ):\n[ L() = -n + _{i=1}^{n} Y_i ]\n\n\nStep 3: Set Derivative to Zero\n[ -n + _{i=1}^{n} Y_i = 0 ]\n[ = _{i=1}^{n} Y_i = {Y} ]\n\n\nConclusion\nThe maximum likelihood estimator (MLE) of ( ) is the sample mean:\n[ _{} = {Y} ]\nThis result makes intuitive sense: in a Poisson distribution, the mean and variance are both equal to ( ), so the best estimate of ( ) from data is the observed average.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python."
  },
  {
    "objectID": "hw2_questions.html#validate-results-using-statsmodels.glm",
    "href": "hw2_questions.html#validate-results-using-statsmodels.glm",
    "title": "Poisson Regression Examples",
    "section": "Validate Results Using statsmodels.GLM()",
    "text": "Validate Results Using statsmodels.GLM()\nTo confirm the accuracy of our manual MLE implementation, we use statsmodels.GLM() to estimate the same Poisson regression model:\n\n\n\n\n\n\n\n\nCode\nimport statsmodels.api as sm\n\n# Drop 'intercept' column and ensure all data is float\nX_glm = X.drop(columns='intercept', errors='ignore').astype(float)\n\n# Add constant for intercept term\nX_glm = sm.add_constant(X_glm)\n\n# Fit GLM model\nglm_model = sm.GLM(Y, X_glm, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary\nglm_results.summary()\n\n### Coefficients and Standard Errors from Poisson Regression\n# Extract coefficient summary\ncoef_table = glm_results.summary2().tables[1][[\"Coef.\", \"Std.Err.\"]]\ncoef_table.rename(columns={\"Coef.\": \"Coefficient\", \"Std.Err.\": \"Std. Error\"}, inplace=True)\n\n# Display table\ncoef_table\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\n\n\n\n\nconst\n-0.508920\n0.183179\n\n\nage\n0.148619\n0.013869\n\n\nage2\n-0.002970\n0.000258\n\n\nNortheast\n0.029170\n0.043625\n\n\nNorthwest\n-0.017575\n0.053781\n\n\nSouth\n0.056561\n0.052662\n\n\nSouthwest\n0.050576\n0.047198\n\n\niscustomer\n0.207591\n0.030895\n\n\n\n\n\n\n\n\n\n\n\nInterpretation of Results\n\nAge has a strong positive effect on patent activity: older firms are more likely to have more patents.\nAge² is negative and significant, suggesting a diminishing return — patent output increases with age but at a decreasing rate.\nBlueprinty’s software has a statistically significant positive coefficient of 0.2076 (p &lt; 0.001). This implies firms using the software are expected to have 23% more patents than comparable non-customers.\nRegional effects (Northeast, Northwest, etc.) are not statistically significant, suggesting location does not materially affect patent outcomes once other factors are controlled for.\n\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "hw2_questions.html#validate-results-using-statsmodels.glm-fixed",
    "href": "hw2_questions.html#validate-results-using-statsmodels.glm-fixed",
    "title": "Poisson Regression Examples",
    "section": "Validate Results Using statsmodels.GLM() (Fixed)",
    "text": "Validate Results Using statsmodels.GLM() (Fixed)\n\nimport statsmodels.api as sm\n\n# Drop 'intercept' column and ensure all data is float\nX_glm = X.drop(columns='intercept', errors='ignore').astype(float)\n\n# Add constant for intercept term\nX_glm = sm.add_constant(X_glm)\n\n# Fit GLM model\nglm_model = sm.GLM(Y, X_glm, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary\nglm_results.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nMon, 05 May 2025\nDeviance:\n2143.3\n\n\nTime:\n10:45:38\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nage\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nage2\n-0.0030\n0.000\n-11.513\n0.000\n-0.003\n-0.002\n\n\nNortheast\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nNorthwest\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nSouth\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nSouthwest\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\niscustomer\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\n\n\n\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "hw2_questions.html#validate-results-using-statsmodels.glm-1",
    "href": "hw2_questions.html#validate-results-using-statsmodels.glm-1",
    "title": "Poisson Regression Examples",
    "section": "Validate Results Using statsmodels.GLM()",
    "text": "Validate Results Using statsmodels.GLM()\n\nimport statsmodels.api as sm\n\n# Drop 'intercept' column and ensure all data is float\nX_glm = X.drop(columns='intercept', errors='ignore').astype(float)\n\n# Add constant for intercept term\nX_glm = sm.add_constant(X_glm)\n\n# Fit GLM model\nglm_model = sm.GLM(Y, X_glm, family=sm.families.Poisson())\nglm_results = glm_model.fit()\n\n# Display summary\nglm_results.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\ny\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nMon, 05 May 2025\nDeviance:\n2143.3\n\n\nTime:\n10:48:27\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nage\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nage2\n-0.0030\n0.000\n-11.513\n0.000\n-0.003\n-0.002\n\n\nNortheast\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nNorthwest\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nSouth\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nSouthwest\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\niscustomer\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\n\n\n\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "hw2_questions.html#estimate-effect-of-blueprintys-software-via-counterfactual-prediction",
    "href": "hw2_questions.html#estimate-effect-of-blueprintys-software-via-counterfactual-prediction",
    "title": "Poisson Regression Examples",
    "section": "Estimate Effect of Blueprinty’s Software via Counterfactual Prediction",
    "text": "Estimate Effect of Blueprinty’s Software via Counterfactual Prediction\nWe simulate two scenarios: - X_0: All firms set to non-customer (iscustomer = 0) - X_1: All firms set to customer (iscustomer = 1)\nThen we compare the predicted number of patents for each firm under the two scenarios using the fitted model.\n\n\nCode\n# Make two versions of X_glm:\n# X_0: all firms are non-customers\n# X_1: all firms are customers\nX_0 = X_glm.copy()\nX_1 = X_glm.copy()\n\nX_0[\"iscustomer\"] = 0\nX_1[\"iscustomer\"] = 1\n\n# Predict expected patent counts\ny_pred_0 = glm_results.predict(X_0)\ny_pred_1 = glm_results.predict(X_1)\n\n# Estimate average treatment effect\naverage_effect = np.mean(y_pred_1 - y_pred_0)\n\n\n\nInterpretation\nThe average difference in predicted number of patents between Blueprinty customers and non-customers is:\n\n\nCode\nprint(f\"Estimated average increase in patent count from using Blueprinty: {average_effect:.3f}\")\n\n\nEstimated average increase in patent count from using Blueprinty: 0.793\n\n\nThis quantifies the effect of Blueprinty’s software: firms using it are predicted to file approximately {average_effect:.3f} more patents over 5 years, on average, than similar firms who don’t use it, controlling for age and region."
  },
  {
    "objectID": "hw2_questions.html#data-cleaning-and-exploratory-data-analysis-eda",
    "href": "hw2_questions.html#data-cleaning-and-exploratory-data-analysis-eda",
    "title": "Poisson Regression Examples",
    "section": "Data Cleaning and Exploratory Data Analysis (EDA)",
    "text": "Data Cleaning and Exploratory Data Analysis (EDA)\nWe begin by dropping listings with missing values in relevant variables, then perform basic EDA on the cleaned dataset.\n\nDrop Rows with Missing Data\n\n\nCode\nimport pandas as pd\ndf = pd.read_csv(\"airbnb.csv\")\nrelevant_cols = [\n    \"number_of_reviews\", \"room_type\", \"bathrooms\", \"bedrooms\", \"price\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\", \"instant_bookable\"\n]\ndf_clean = df[relevant_cols].dropna()\n\n\n\n\nDistribution of Number of Reviews\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(8, 6))\nsns.histplot(df_clean[\"number_of_reviews\"], bins=50, kde=False)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count of Listings\")\nplt.xlim(0, 100)  \nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAverage Number of Reviews by Room Type\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n# Data\navg_reviews = df_clean.groupby(\"room_type\")[\"number_of_reviews\"].mean().reset_index()\n\n# Better-looking custom blue palette\ncustom_blue_palette = [\"#A6CEE3\", \"#1F78B4\", \"#08519C\"]\n\n# Plot with warning suppression\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\", category=FutureWarning)\n\n    plt.figure(figsize=(8, 6))\n    sns.barplot(\n        data=avg_reviews,\n        x=\"room_type\",\n        y=\"number_of_reviews\",\n        palette=custom_blue_palette,\n    )\n\nplt.title(\"Average Number of Reviews by Room Type\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Room Type\", fontsize=12)\nplt.ylabel(\"Average Number of Reviews\", fontsize=12)\nplt.xticks(rotation=0)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nAverage Number of Reviews by Instant Bookability\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\navg_reviews_by_bookable = df_clean.groupby(\"instant_bookable\")[\"number_of_reviews\"].mean().reset_index()\navg_reviews_by_bookable[\"instant_bookable\"] = avg_reviews_by_bookable[\"instant_bookable\"].map({\"f\": \"No\", \"t\": \"Yes\"})\n\nblue_palette = {\"No\": \"#6baed6\", \"Yes\": \"#2171b5\"}\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n\n    plt.figure(figsize=(8, 6))\n    sns.barplot(\n        data=avg_reviews_by_bookable,\n        x=\"instant_bookable\",\n        y=\"number_of_reviews\",\n        hue=\"instant_bookable\", \n        palette=blue_palette,\n        legend=False,\n    )\nplt.title(\"Average Number of Reviews by Instant Bookability\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Instant Bookable\", fontsize=12)\nplt.ylabel(\"Average Number of Reviews\", fontsize=12)\nplt.ylim(0, avg_reviews_by_bookable[\"number_of_reviews\"].max() * 1.1)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCorrelation with Numeric Predictors\n\n\nCode\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Select numeric variables\nnumeric_vars = [\n    \"number_of_reviews\", \"bathrooms\", \"bedrooms\", \"price\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"\n]\n\n# Compute correlation matrix\ncorrelation_matrix = df_clean[numeric_vars].corr()\n\n# Create the heatmap in blue shades\nplt.figure(figsize=(10, 8))\nsns.heatmap(\n    correlation_matrix,\n    annot=True,\n    fmt=\".2f\",\n    cmap=\"Blues\",             \n    vmin=0, vmax=1,          \n    square=True,\n    linewidths=0.75,\n    linecolor=\"white\",\n    annot_kws={\"fontsize\": 10, \"weight\": \"bold\"}\n)\n\nplt.title(\"Correlation Matrix of Numeric Variables\", fontsize=14, weight=\"bold\")\nplt.xticks(rotation=45, ha=\"right\", fontsize=10)\nplt.yticks(rotation=0, fontsize=10)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Model Using statsmodels.GLM()\nWe model the number of reviews (as a proxy for bookings) using a Poisson regression with the following predictors: - room_type (categorical) - instant_bookable (binary) - price, days, bathrooms, bedrooms - Review scores: cleanliness, location, value\n\n\nCode\nimport statsmodels.api as sm\n\nroom_dummies = pd.get_dummies(df_clean[\"room_type\"], drop_first=True)\ndf_clean[\"instant_bookable\"] = df_clean[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\n# Create design matrix\nX = pd.concat([\n    df_clean[[\"price\", \"days\", \"bathrooms\", \"bedrooms\",\n              \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n              \"instant_bookable\"]],\n    room_dummies\n], axis=1)\n\nX = sm.add_constant(X)\nX = X.astype(float)  \n\nY = df_clean[\"number_of_reviews\"]\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\nsummary_df = poisson_results.summary2().tables[1]\n\n# Rename columns for clarity\nsummary_df = summary_df.rename(columns={\n    \"Coef.\": \"Coefficient\",\n    \"Std.Err.\": \"Std. Error\",\n    \"P&gt;|z|\": \"P-Value\"\n})\n\n# Filter to statistically significant predictors (p &lt; 0.05)\nsignificant_results = summary_df[summary_df[\"P-Value\"] &lt; 0.05][[\"Coefficient\", \"Std. Error\", \"P-Value\"]]\n\n# Round for display\nsignificant_results = significant_results.round(4)\n\n# Display\nsignificant_results\n\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nP-Value\n\n\n\n\nconst\n3.4980\n0.0161\n0.0000\n\n\nprice\n-0.0000\n0.0000\n0.0315\n\n\ndays\n0.0001\n0.0000\n0.0000\n\n\nbathrooms\n-0.1177\n0.0037\n0.0000\n\n\nbedrooms\n0.0741\n0.0020\n0.0000\n\n\nreview_scores_cleanliness\n0.1131\n0.0015\n0.0000\n\n\nreview_scores_location\n-0.0769\n0.0016\n0.0000\n\n\nreview_scores_value\n-0.0911\n0.0018\n0.0000\n\n\ninstant_bookable\n0.3459\n0.0029\n0.0000\n\n\nPrivate room\n-0.0105\n0.0027\n0.0001\n\n\nShared room\n-0.2463\n0.0086\n0.0000\n\n\n\n\n\n\n\n\n\nInterpretation"
  },
  {
    "objectID": "hw2_questions.html#poisson-regression-model-using-statsmodels.glm",
    "href": "hw2_questions.html#poisson-regression-model-using-statsmodels.glm",
    "title": "Poisson Regression Examples",
    "section": "Poisson Regression Model Using statsmodels.GLM()",
    "text": "Poisson Regression Model Using statsmodels.GLM()\nWe model the number of reviews (as a proxy for bookings) using a Poisson regression with the following predictors: - room_type (categorical) - instant_bookable (binary) - price, days, bathrooms, bedrooms - Review scores: cleanliness, location, value\n\nPrepare Data for Modeling\n\n\nCode\nimport statsmodels.api as sm\n\nroom_dummies = pd.get_dummies(df_clean[\"room_type\"], drop_first=True)\ndf_clean[\"instant_bookable\"] = df_clean[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\n# Create design matrix\nX = pd.concat([\n    df_clean[[\"price\", \"days\", \"bathrooms\", \"bedrooms\",\n              \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n              \"instant_bookable\"]],\n    room_dummies\n], axis=1)\n\nX = sm.add_constant(X)\nX = X.astype(float)  \n\nY = df_clean[\"number_of_reviews\"]\n\n# Fit the Poisson regression model\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Display summary\npoisson_results.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nnumber_of_reviews\nNo. Observations:\n30160\n\n\nModel:\nGLM\nDf Residuals:\n30149\n\n\nModel Family:\nPoisson\nDf Model:\n10\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-5.2418e+05\n\n\nDate:\nTue, 06 May 2025\nDeviance:\n9.2689e+05\n\n\nTime:\n08:32:41\nPearson chi2:\n1.37e+06\n\n\nNo. Iterations:\n10\nPseudo R-squ. (CS):\n0.6840\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n3.4980\n0.016\n217.396\n0.000\n3.467\n3.530\n\n\nprice\n-1.791e-05\n8.33e-06\n-2.151\n0.031\n-3.42e-05\n-1.59e-06\n\n\ndays\n5.072e-05\n3.91e-07\n129.755\n0.000\n5e-05\n5.15e-05\n\n\nbathrooms\n-0.1177\n0.004\n-31.394\n0.000\n-0.125\n-0.110\n\n\nbedrooms\n0.0741\n0.002\n37.197\n0.000\n0.070\n0.078\n\n\nreview_scores_cleanliness\n0.1131\n0.001\n75.611\n0.000\n0.110\n0.116\n\n\nreview_scores_location\n-0.0769\n0.002\n-47.796\n0.000\n-0.080\n-0.074\n\n\nreview_scores_value\n-0.0911\n0.002\n-50.490\n0.000\n-0.095\n-0.088\n\n\ninstant_bookable\n0.3459\n0.003\n119.666\n0.000\n0.340\n0.352\n\n\nPrivate room\n-0.0105\n0.003\n-3.847\n0.000\n-0.016\n-0.005\n\n\nShared room\n-0.2463\n0.009\n-28.578\n0.000\n-0.263\n-0.229\n\n\n\n\n\n\n\n\nInterpretation\nFrom the model output, we can interpret coefficients as follows: - Positive coefficients indicate variables associated with a higher expected number of reviews (bookings). - For example, if instant_bookable = 1 has a positive and significant coefficient, it suggests instant booking increases booking activity.\nLet me know if you want help interpreting the coefficients or creating predicted booking counts for different scenarios."
  },
  {
    "objectID": "hw2_questions.html#average-number-of-reviews-by-room-type-bar-chart",
    "href": "hw2_questions.html#average-number-of-reviews-by-room-type-bar-chart",
    "title": "Poisson Regression Examples",
    "section": "Average Number of Reviews by Room Type (Bar Chart)",
    "text": "Average Number of Reviews by Room Type (Bar Chart)\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\navg_reviews = df_clean.groupby(\"room_type\")[\"number_of_reviews\"].mean().reset_index()\nplt.figure(figsize=(8, 6))\nsns.barplot(data=avg_reviews, x=\"room_type\", y=\"number_of_reviews\", palette=\"viridis\")\nplt.title(\"Average Number of Reviews by Room Type\")\nplt.xlabel(\"Room Type\")\nplt.ylabel(\"Average Number of Reviews\")\nplt.show()\n\n\n/tmp/ipykernel_80510/1791832712.py:6: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n\nBar Chart: Average Number of Reviews by Instant Bookability\n\n\nCode\navg_reviews_by_bookable = df_clean.groupby(\"instant_bookable\")[\"number_of_reviews\"].mean().reset_index()\navg_reviews_by_bookable[\"instant_bookable\"] = avg_reviews_by_bookable[\"instant_bookable\"].map({\"t\": \"Yes\", \"f\": \"No\"})\nplt.figure(figsize=(6, 5))\nsns.barplot(data=avg_reviews_by_bookable, x=\"instant_bookable\", y=\"number_of_reviews\", palette=\"Set2\")\nplt.title(\"Average Number of Reviews by Instant Bookability\")\nplt.xlabel(\"Instant Bookable\")\nplt.ylabel(\"Average Number of Reviews\")\nplt.ylim(0, avg_reviews_by_bookable[\"number_of_reviews\"].max() * 1.1)\nplt.show()\n\n\n/tmp/ipykernel_80510/393136636.py:4: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation with Numeric Predictors\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Select numeric variables\nnumeric_vars = [\n    \"number_of_reviews\", \"bathrooms\", \"bedrooms\", \"price\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"\n]\n\n# Compute correlation matrix\ncorrelation_matrix = df_clean[numeric_vars].corr()\n\n# Create heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(\n    correlation_matrix,\n    annot=True,\n    fmt=\".2f\",\n    cmap=\"coolwarm\",\n    square=True,\n    linewidths=0.5,\n    cbar_kws={\"shrink\": 0.8},\n    annot_kws={\"size\": 10}\n)\n\nplt.title(\"Correlation Matrix of Numeric Variables\", fontsize=14, weight=\"bold\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "hw2_questions.html#probability-mass-function",
    "href": "hw2_questions.html#probability-mass-function",
    "title": "Poisson Regression Examples",
    "section": "Probability Mass Function",
    "text": "Probability Mass Function\nThe probability mass function for a single observation from a Poisson distribution is:\n\\[\nf(Y_i \\mid \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]"
  },
  {
    "objectID": "hw2_questions.html#likelihood-function",
    "href": "hw2_questions.html#likelihood-function",
    "title": "Poisson Regression Examples",
    "section": "Likelihood Function",
    "text": "Likelihood Function\nAssuming the observations are independent, the likelihood function for the entire dataset is:\n\\[\nL(\\lambda; Y_1, \\ldots, Y_n) = \\prod_{i=1}^{n} \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\nThis can be rewritten as:\n\\[\nL(\\lambda) = e^{-n\\lambda} \\cdot \\lambda^{\\sum_{i=1}^{n} Y_i} \\cdot \\prod_{i=1}^{n} \\frac{1}{Y_i!}\n\\]"
  },
  {
    "objectID": "hw2_questions.html#log-likelihood-function",
    "href": "hw2_questions.html#log-likelihood-function",
    "title": "Poisson Regression Examples",
    "section": "Log-Likelihood Function",
    "text": "Log-Likelihood Function\nTaking the natural logarithm of the likelihood gives:\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log(Y_i!)\n\\]\nThis log-likelihood will be used to estimate ( ) via Maximum Likelihood Estimation (MLE).\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define the Poisson log-likelihood function\ndef poisson_loglikelihood(lambd, Y):\n    if lambd &lt;= 0:\n        return -np.inf  # log-likelihood is undefined for non-positive lambda\n    return np.sum(-lambd + Y * np.log(lambd) - gammaln(Y + 1))\n\n\n\n\n\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\nWe visualize the Poisson log-likelihood as a function of ( ), where the maximum corresponds to the MLE.\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\n# Example data: simulated Poisson observations\nY_sample = df[\"patents\"].values\n\n# Range of lambda values to plot\nlambdas = np.linspace(0.1, 10, 300)\nlog_likelihoods = [poisson_loglikelihood(l, Y_sample) for l in lambdas]\n\n# Find MLE visually\nlambda_mle = lambdas[np.argmax(log_likelihoods)]\n\n# Plot\nplt.figure(figsize=(8, 6))\nplt.plot(lambdas, log_likelihoods, linewidth=2, color=\"steelblue\")\nplt.axvline(lambda_mle, color=\"darkorange\", linestyle=\"--\", label=f\"MLE ≈ {lambda_mle:.2f}\")\nplt.title(\"Poisson Log-Likelihood vs. Lambda\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Lambda\", fontsize=12)\nplt.ylabel(\"Log-Likelihood\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nDerivation of the MLE for Poisson Distribution\nSuppose ( Y_1, Y_2, , Y_n () ), where the probability mass function is: \\[\n\\[\nf(Y_i | \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\n\\]\n\nStep 1: Log-Likelihood Function\nThe log-likelihood of the entire sample is: \\[\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\n\\]\nWe can simplify this (since ( Y_i! ) does not depend on ( )): \\[\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda + \\text{constant}\n\\]\n\\] #### Step 2: First Derivative\nTake the derivative with respect to ( ): \\[\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i\n\\]\n\\] #### Step 3: Set Derivative to Zero \\[\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n\\]\n\\] \\[\n\\[\n\\Rightarrow \\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\n\\] #### Conclusion\nThe maximum likelihood estimator (MLE) of ( ) is the sample mean: \\[\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\n\\]\nThis result makes intuitive sense: in a Poisson distribution, the mean and variance are both equal to ( ), so the best estimate of ( ) from data is the observed average."
  },
  {
    "objectID": "hw2_questions.html#log-likelihood-function-1",
    "href": "hw2_questions.html#log-likelihood-function-1",
    "title": "Poisson Regression Examples",
    "section": "Log-Likelihood Function",
    "text": "Log-Likelihood Function\nTaking the natural logarithm of the likelihood function gives the log-likelihood:\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda - \\sum_{i=1}^{n} \\log(Y_i!)\n\\]\nThis log-likelihood will be used to estimate ( ) via Maximum Likelihood Estimation (MLE).\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define the Poisson log-likelihood function\ndef poisson_loglikelihood(lambd, Y):\n    if lambd &lt;= 0:\n        return -np.inf  # log-likelihood is undefined for non-positive lambda\n    return np.sum(-lambd + Y * np.log(lambd) - gammaln(Y + 1))\n\n\n\n\n\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\nWe visualize the Poisson log-likelihood as a function of ( ), where the maximum corresponds to the MLE.\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\n# Example data: simulated Poisson observations\nY_sample = df[\"patents\"].values\n\n# Range of lambda values to plot\nlambdas = np.linspace(0.1, 10, 300)\nlog_likelihoods = [poisson_loglikelihood(l, Y_sample) for l in lambdas]\n\n# Find MLE visually\nlambda_mle = lambdas[np.argmax(log_likelihoods)]\n\n# Plot\nplt.figure(figsize=(8, 6))\nplt.plot(lambdas, log_likelihoods, linewidth=2, color=\"steelblue\")\nplt.axvline(lambda_mle, color=\"darkorange\", linestyle=\"--\", label=f\"MLE ≈ {lambda_mle:.2f}\")\nplt.title(\"Poisson Log-Likelihood vs. Lambda\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Lambda\", fontsize=12)\nplt.ylabel(\"Log-Likelihood\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nDerivation of the MLE for Poisson Distribution\nSuppose ( Y_1, Y_2, , Y_n () ), where the probability mass function is: \\[\n\\[\nf(Y_i | \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\n\\]\n\nStep 1: Log-Likelihood Function\nThe log-likelihood of the entire sample is: \\[\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\n\\]\nWe can simplify this (since ( Y_i! ) does not depend on ( )): \\[\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda + \\text{constant}\n\\]\n\\] #### Step 2: First Derivative\nTake the derivative with respect to ( ): \\[\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i\n\\]\n\\] #### Step 3: Set Derivative to Zero \\[\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n\\]\n\\] \\[\n\\[\n\\Rightarrow \\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\n\\] #### Conclusion\nThe maximum likelihood estimator (MLE) of ( ) is the sample mean: \\[\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\n\\]\nThis result makes intuitive sense: in a Poisson distribution, the mean and variance are both equal to ( ), so the best estimate of ( ) from data is the observed average."
  },
  {
    "objectID": "hw2_questions.html#this-log-likelihood-will-be-used-to-estimate-via-maximum-likelihood-estimation-mle.",
    "href": "hw2_questions.html#this-log-likelihood-will-be-used-to-estimate-via-maximum-likelihood-estimation-mle.",
    "title": "Poisson Regression Examples",
    "section": "This log-likelihood will be used to estimate ( ) via Maximum Likelihood Estimation (MLE).",
    "text": "This log-likelihood will be used to estimate ( ) via Maximum Likelihood Estimation (MLE).\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nfrom scipy.special import gammaln\n\n# Define the Poisson log-likelihood function\ndef poisson_loglikelihood(lambd, Y):\n    if lambd &lt;= 0:\n        return -np.inf  # log-likelihood is undefined for non-positive lambda\n    return np.sum(-lambd + Y * np.log(lambd) - gammaln(Y + 1))\n\n\n\n\n\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\nWe visualize the Poisson log-likelihood as a function of ( ), where the maximum corresponds to the MLE.\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\n\n# Example data: simulated Poisson observations\nY_sample = df[\"patents\"].values\n\n# Range of lambda values to plot\nlambdas = np.linspace(0.1, 10, 300)\nlog_likelihoods = [poisson_loglikelihood(l, Y_sample) for l in lambdas]\n\n# Find MLE visually\nlambda_mle = lambdas[np.argmax(log_likelihoods)]\n\n# Plot\nplt.figure(figsize=(8, 6))\nplt.plot(lambdas, log_likelihoods, linewidth=2, color=\"steelblue\")\nplt.axvline(lambda_mle, color=\"darkorange\", linestyle=\"--\", label=f\"MLE ≈ {lambda_mle:.2f}\")\nplt.title(\"Poisson Log-Likelihood vs. Lambda\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Lambda\", fontsize=12)\nplt.ylabel(\"Log-Likelihood\", fontsize=12)\nplt.grid(True, linestyle=\"--\", alpha=0.5)\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\nDerivation of the MLE for Poisson Distribution\nSuppose ( Y_1, Y_2, , Y_n () ), where the probability mass function is: \\[\n\\[\nf(Y_i | \\lambda) = \\frac{e^{-\\lambda} \\lambda^{Y_i}}{Y_i!}\n\\]\n\\]\n\nStep 1: Log-Likelihood Function\nThe log-likelihood of the entire sample is: \\[\n\\[\n\\log L(\\lambda) = \\sum_{i=1}^{n} \\left( -\\lambda + Y_i \\log \\lambda - \\log Y_i! \\right)\n\\]\n\\]\nWe can simplify this (since ( Y_i! ) does not depend on ( )): \\[\n\\[\n\\log L(\\lambda) = -n\\lambda + \\left( \\sum_{i=1}^{n} Y_i \\right) \\log \\lambda + \\text{constant}\n\\]\n\\] #### Step 2: First Derivative\nTake the derivative with respect to ( ): \\[\n\\[\n\\frac{d}{d\\lambda} \\log L(\\lambda) = -n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i\n\\]\n\\] #### Step 3: Set Derivative to Zero \\[\n\\[\n-n + \\frac{1}{\\lambda} \\sum_{i=1}^{n} Y_i = 0\n\\]\n\\] \\[\n\\[\n\\Rightarrow \\lambda = \\frac{1}{n} \\sum_{i=1}^{n} Y_i = \\bar{Y}\n\\]\n\\] #### Conclusion\nThe maximum likelihood estimator (MLE) of ( ) is the sample mean: \\[\n\\[\n\\hat{\\lambda}_{\\text{MLE}} = \\bar{Y}\n\\]\n\\]\nThis result makes intuitive sense: in a Poisson distribution, the mean and variance are both equal to ( ), so the best estimate of ( ) from data is the observed average."
  },
  {
    "objectID": "hw2_questions.html#average-number-of-reviews-by-room-type-bar-chart-1",
    "href": "hw2_questions.html#average-number-of-reviews-by-room-type-bar-chart-1",
    "title": "Poisson Regression Examples",
    "section": "Average Number of Reviews by Room Type (Bar Chart)",
    "text": "Average Number of Reviews by Room Type (Bar Chart)\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Prepare data\navg_reviews = df_clean.groupby(\"room_type\")[\"number_of_reviews\"].mean().reset_index()\n\n# Custom consistent palette for room types\nroom_palette = {\n    \"Entire home/apt\": \"#4C72B0\",   # Blue\n    \"Private room\": \"#DD8452\",      # Orange\n    \"Shared room\": \"#55A868\"        # Green\n}\n\n# Plot\nplt.figure(figsize=(8, 6))\nsns.barplot(\n    data=avg_reviews,\n    x=\"room_type\",\n    y=\"number_of_reviews\",\n    palette=room_palette,\n    edgecolor=\"black\"\n)\n\nplt.title(\"Average Number of Reviews by Room Type\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Room Type\", fontsize=12)\nplt.ylabel(\"Average Number of Reviews\", fontsize=12)\nplt.xticks(rotation=0)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n/tmp/ipykernel_23669/2484720183.py:16: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n\nBar Chart: Average Number of Reviews by Instant Bookability\n\n\nCode\navg_reviews_by_bookable = df_clean.groupby(\"instant_bookable\")[\"number_of_reviews\"].mean().reset_index()\navg_reviews_by_bookable[\"instant_bookable\"] = avg_reviews_by_bookable[\"instant_bookable\"].map({\"t\": \"Yes\", \"f\": \"No\"})\nplt.figure(figsize=(8,6))\nsns.barplot(data=avg_reviews_by_bookable, x=\"instant_bookable\", y=\"number_of_reviews\", palette=\"Set2\")\nplt.title(\"Average Number of Reviews by Instant Bookability\")\nplt.xlabel(\"Instant Bookable\")\nplt.ylabel(\"Average Number of Reviews\")\nplt.ylim(0, avg_reviews_by_bookable[\"number_of_reviews\"].max() * 1.1)\nplt.show()\n\n\n/tmp/ipykernel_23669/1164119133.py:4: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n\n\n\nCorrelation with Numeric Predictors\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Select numeric variables\nnumeric_vars = [\n    \"number_of_reviews\", \"bathrooms\", \"bedrooms\", \"price\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"\n]\n\n# Compute correlation matrix\ncorrelation_matrix = df_clean[numeric_vars].corr()\n\n# Create heatmap\nplt.figure(figsize=(8,6))\nsns.heatmap(\n    correlation_matrix,\n    annot=True,\n    fmt=\".2f\",\n    cmap=\"coolwarm\",\n    square=True,\n    linewidths=0.5,\n    cbar_kws={\"shrink\": 0.8},\n    annot_kws={\"size\": 10}\n)\n\nplt.title(\"Correlation Matrix of Numeric Variables\", fontsize=14, weight=\"bold\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Model Using statsmodels.GLM()\nWe model the number of reviews (as a proxy for bookings) using a Poisson regression with the following predictors: - room_type (categorical) - instant_bookable (binary) - price, days, bathrooms, bedrooms - Review scores: cleanliness, location, value\n\n\nCode\nimport statsmodels.api as sm\n\nroom_dummies = pd.get_dummies(df_clean[\"room_type\"], drop_first=True)\ndf_clean[\"instant_bookable\"] = df_clean[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\n# Create design matrix\nX = pd.concat([\n    df_clean[[\"price\", \"days\", \"bathrooms\", \"bedrooms\",\n              \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n              \"instant_bookable\"]],\n    room_dummies\n], axis=1)\n\nX = sm.add_constant(X)\nX = X.astype(float)  \n\nY = df_clean[\"number_of_reviews\"]\n\n# Fit the Poisson regression model\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Display summary\npoisson_results.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nnumber_of_reviews\nNo. Observations:\n30160\n\n\nModel:\nGLM\nDf Residuals:\n30149\n\n\nModel Family:\nPoisson\nDf Model:\n10\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-5.2418e+05\n\n\nDate:\nWed, 07 May 2025\nDeviance:\n9.2689e+05\n\n\nTime:\n14:34:59\nPearson chi2:\n1.37e+06\n\n\nNo. Iterations:\n10\nPseudo R-squ. (CS):\n0.6840\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n3.4980\n0.016\n217.396\n0.000\n3.467\n3.530\n\n\nprice\n-1.791e-05\n8.33e-06\n-2.151\n0.031\n-3.42e-05\n-1.59e-06\n\n\ndays\n5.072e-05\n3.91e-07\n129.755\n0.000\n5e-05\n5.15e-05\n\n\nbathrooms\n-0.1177\n0.004\n-31.394\n0.000\n-0.125\n-0.110\n\n\nbedrooms\n0.0741\n0.002\n37.197\n0.000\n0.070\n0.078\n\n\nreview_scores_cleanliness\n0.1131\n0.001\n75.611\n0.000\n0.110\n0.116\n\n\nreview_scores_location\n-0.0769\n0.002\n-47.796\n0.000\n-0.080\n-0.074\n\n\nreview_scores_value\n-0.0911\n0.002\n-50.490\n0.000\n-0.095\n-0.088\n\n\ninstant_bookable\n0.3459\n0.003\n119.666\n0.000\n0.340\n0.352\n\n\nPrivate room\n-0.0105\n0.003\n-3.847\n0.000\n-0.016\n-0.005\n\n\nShared room\n-0.2463\n0.009\n-28.578\n0.000\n-0.263\n-0.229\n\n\n\n\n\n\n\n\nInterpretation\nFrom the model output, we can interpret coefficients as follows: - Positive coefficients indicate variables associated with a higher expected number of reviews (bookings). - For example, if instant_bookable = 1 has a positive and significant coefficient, it suggests instant booking increases booking activity."
  },
  {
    "objectID": "hw2_questions.html#average-number-of-reviews-by-room-type",
    "href": "hw2_questions.html#average-number-of-reviews-by-room-type",
    "title": "Poisson Regression Examples",
    "section": "Average Number of Reviews by Room Type",
    "text": "Average Number of Reviews by Room Type\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\n\n# Prepare data\navg_reviews = df_clean.groupby(\"room_type\")[\"number_of_reviews\"].mean().reset_index()\n\n# Define a blue palette using seaborn built-in or custom shades\nblue_shades = sns.color_palette(\"Blues_d\", n_colors=len(avg_reviews))\n\n# Plot safely (suppress warning)\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\", category=FutureWarning)\n\n    plt.figure(figsize=(8, 6))\n    sns.barplot(\n        data=avg_reviews,\n        x=\"room_type\",\n        y=\"number_of_reviews\",\n        palette=blue_shades\n    )\n\nplt.title(\"Average Number of Reviews by Room Type\", fontsize=14, weight=\"bold\")\nplt.xlabel(\"Room Type\", fontsize=12)\nplt.ylabel(\"Average Number of Reviews\", fontsize=12)\nplt.xticks(rotation=0)\nplt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n### Bar Chart: Average Number of Reviews by Instant Bookability\n\n::: {#63740fc3 .cell execution_count=19}\n``` {.python .cell-code}\navg_reviews_by_bookable = df_clean.groupby(\"instant_bookable\")[\"number_of_reviews\"].mean().reset_index()\navg_reviews_by_bookable[\"instant_bookable\"] = avg_reviews_by_bookable[\"instant_bookable\"].map({\"t\": \"Yes\", \"f\": \"No\"})\nplt.figure(figsize=(8,6))\nsns.barplot(data=avg_reviews_by_bookable, x=\"instant_bookable\", y=\"number_of_reviews\", palette=\"Set2\")\nplt.title(\"Average Number of Reviews by Instant Bookability\")\nplt.xlabel(\"Instant Bookable\")\nplt.ylabel(\"Average Number of Reviews\")\nplt.ylim(0, avg_reviews_by_bookable[\"number_of_reviews\"].max() * 1.1)\nplt.show()\n\n/tmp/ipykernel_26213/1164119133.py:4: FutureWarning:\n\n\n\nPassing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n\n\n\n\n\n\n\n\n\n\n:::\n\nCorrelation with Numeric Predictors\n\n\nCode\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Select numeric variables\nnumeric_vars = [\n    \"number_of_reviews\", \"bathrooms\", \"bedrooms\", \"price\", \"days\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\"\n]\n\n# Compute correlation matrix\ncorrelation_matrix = df_clean[numeric_vars].corr()\n\n# Create heatmap\nplt.figure(figsize=(8,6))\nsns.heatmap(\n    correlation_matrix,\n    annot=True,\n    fmt=\".2f\",\n    cmap=\"coolwarm\",\n    square=True,\n    linewidths=0.5,\n    cbar_kws={\"shrink\": 0.8},\n    annot_kws={\"size\": 10}\n)\n\nplt.title(\"Correlation Matrix of Numeric Variables\", fontsize=14, weight=\"bold\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Model Using statsmodels.GLM()\nWe model the number of reviews (as a proxy for bookings) using a Poisson regression with the following predictors: - room_type (categorical) - instant_bookable (binary) - price, days, bathrooms, bedrooms - Review scores: cleanliness, location, value\n\n\nCode\nimport statsmodels.api as sm\n\nroom_dummies = pd.get_dummies(df_clean[\"room_type\"], drop_first=True)\ndf_clean[\"instant_bookable\"] = df_clean[\"instant_bookable\"].map({\"t\": 1, \"f\": 0})\n\n# Create design matrix\nX = pd.concat([\n    df_clean[[\"price\", \"days\", \"bathrooms\", \"bedrooms\",\n              \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\",\n              \"instant_bookable\"]],\n    room_dummies\n], axis=1)\n\nX = sm.add_constant(X)\nX = X.astype(float)  \n\nY = df_clean[\"number_of_reviews\"]\n\n# Fit the Poisson regression model\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Display summary\npoisson_results.summary()\n\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nnumber_of_reviews\nNo. Observations:\n30160\n\n\nModel:\nGLM\nDf Residuals:\n30149\n\n\nModel Family:\nPoisson\nDf Model:\n10\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-5.2418e+05\n\n\nDate:\nWed, 07 May 2025\nDeviance:\n9.2689e+05\n\n\nTime:\n14:38:22\nPearson chi2:\n1.37e+06\n\n\nNo. Iterations:\n10\nPseudo R-squ. (CS):\n0.6840\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n3.4980\n0.016\n217.396\n0.000\n3.467\n3.530\n\n\nprice\n-1.791e-05\n8.33e-06\n-2.151\n0.031\n-3.42e-05\n-1.59e-06\n\n\ndays\n5.072e-05\n3.91e-07\n129.755\n0.000\n5e-05\n5.15e-05\n\n\nbathrooms\n-0.1177\n0.004\n-31.394\n0.000\n-0.125\n-0.110\n\n\nbedrooms\n0.0741\n0.002\n37.197\n0.000\n0.070\n0.078\n\n\nreview_scores_cleanliness\n0.1131\n0.001\n75.611\n0.000\n0.110\n0.116\n\n\nreview_scores_location\n-0.0769\n0.002\n-47.796\n0.000\n-0.080\n-0.074\n\n\nreview_scores_value\n-0.0911\n0.002\n-50.490\n0.000\n-0.095\n-0.088\n\n\ninstant_bookable\n0.3459\n0.003\n119.666\n0.000\n0.340\n0.352\n\n\nPrivate room\n-0.0105\n0.003\n-3.847\n0.000\n-0.016\n-0.005\n\n\nShared room\n-0.2463\n0.009\n-28.578\n0.000\n-0.263\n-0.229\n\n\n\n\n\n\n\n\nInterpretation\nFrom the model output, we can interpret coefficients as follows: - Positive coefficients indicate variables associated with a higher expected number of reviews (bookings). - For example, if instant_bookable = 1 has a positive and significant coefficient, it suggests instant booking increases booking activity."
  },
  {
    "objectID": "hw2_questions.html#interpretation-of-statistically-significant-coefficients",
    "href": "hw2_questions.html#interpretation-of-statistically-significant-coefficients",
    "title": "Poisson Regression Examples",
    "section": "Interpretation of Statistically Significant Coefficients",
    "text": "Interpretation of Statistically Significant Coefficients\n\nIntercept (3.4980)\nThe baseline log-expected number of reviews for a listing when all other variables are zero (serves as a reference point).\nPrice (-0.0000)\nAs price increases, the expected number of reviews decreases slightly. This effect is small but statistically significant, indicating higher-priced listings may deter some bookings.\nDays Active (+0.0001)\nListings that have been active longer tend to accumulate more reviews. This reflects more exposure over time.\nBathrooms (-0.1177)\nSurprisingly, listings with more bathrooms tend to receive fewer reviews. This might reflect that larger or luxury properties are booked less frequently.\nBedrooms (+0.0741)\nListings with more bedrooms attract more bookings, likely due to their ability to accommodate larger groups.\nCleanliness Score (+0.1131)\nClean listings receive more reviews, reinforcing the importance of cleanliness to guests.\nLocation Score (-0.0769)\nA negative association with reviews, possibly due to limited variation in location ratings or multicollinearity with other variables.\nValue Score (-0.0911)\nHigher value scores are associated with fewer reviews, which may reflect that guests leave high value ratings in less competitive or less popular markets.\nInstant Bookable (+0.3459)\nListings that support instant booking are expected to receive approximately 41% more reviews than those that do not:\n[ (0.3459) ]\nPrivate Room (-0.0105)\nPrivate rooms receive slightly fewer reviews than entire homes, likely due to lower demand or guest preferences for full space.\nShared Room (-0.2463)\nShared rooms receive significantly fewer reviews — about 22% fewer than entire homes:\n[ (-0.2463) ]"
  },
  {
    "objectID": "hw3_questions.html",
    "href": "hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "hw3_questions.html#simulate-conjoint-data",
    "href": "hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nCode\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "hw3_questions.html#preparing-the-data-for-estimation",
    "href": "hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nCode\n# Load necessary package\n# install.packages(\"dplyr\") # Run this once if not already installed\nlibrary(dplyr)\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\n# Load the dataset\ndf &lt;- read.csv(\"conjoint_data.csv\")\n\n# Create a unique identifier for each choice situation (respondent-task combination)\ndf$resp_task &lt;- paste(df$resp, df$task, sep = \"_\")\n\n# One-hot encode categorical variables: brand and ad\n# Reference levels: brand H, ad No\ndf$brand_N &lt;- ifelse(df$brand == \"N\", 1, 0)\ndf$brand_P &lt;- ifelse(df$brand == \"P\", 1, 0)\ndf$ad_Yes  &lt;- ifelse(df$ad == \"Yes\", 1, 0)\n\n# Sort the data for clean indexing\ndf &lt;- df %&gt;% arrange(resp, task)\n\n# Create an alternative ID for each profile within a task\ndf &lt;- df %&gt;%\n  group_by(resp, task) %&gt;%\n  mutate(alt_id = row_number() - 1) %&gt;%\n  ungroup()\n\n# View the reshaped data\nhead(df)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nresp\ntask\nchoice\nbrand\nad\nprice\nresp_task\nbrand_N\nbrand_P\nad_Yes\nalt_id\n\n\n\n\n1\n1\n1\nN\nYes\n28\n1_1\n1\n0\n1\n0\n\n\n1\n1\n0\nH\nYes\n16\n1_1\n0\n0\n1\n1\n\n\n1\n1\n0\nP\nYes\n16\n1_1\n0\n1\n1\n2\n\n\n1\n2\n0\nN\nYes\n32\n1_2\n1\n0\n1\n0\n\n\n1\n2\n1\nP\nYes\n16\n1_2\n0\n1\n1\n1\n\n\n1\n2\n0\nN\nYes\n24\n1_2\n1\n0\n1\n2"
  },
  {
    "objectID": "hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nCode\n# Load required packages\nlibrary(dplyr)\n\n# Load and prepare data\ndf &lt;- read.csv(\"conjoint_data.csv\")\n\n# Create a unique identifier for each choice task\ndf$resp_task &lt;- paste(df$res, df$task, sep = \"_\")\n\n# Convert categorical variables to dummy variables (reference: brand H, ad No)\ndf &lt;- df %&gt;%\n  mutate(\n    brand_N = ifelse(brand == \"N\", 1, 0),\n    brand_P = ifelse(brand == \"P\", 1, 0),\n    ad_Yes  = ifelse(ad == \"Yes\", 1, 0)\n  ) %&gt;%\n  arrange(resp, task)\n\n# Create X matrix and choice vector y\nX &lt;- as.matrix(df[, c(\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\")])\ny &lt;- df$choice\ngroup_sizes &lt;- table(df$resp_task)\n\n# Define negative log-likelihood function\nneg_log_likelihood &lt;- function(beta) {\n  utilities &lt;- X %*% beta\n  start &lt;- 1\n  loglik &lt;- 0\n  for (size in group_sizes) {\n    end &lt;- start + size - 1\n    u_task &lt;- utilities[start:end]\n    y_task &lt;- y[start:end]\n    exp_u &lt;- exp(u_task - max(u_task))  # numerical stability\n    prob &lt;- exp_u / sum(exp_u)\n    chosen_prob &lt;- prob[which(y_task == 1)]\n    loglik &lt;- loglik + log(chosen_prob)\n    start &lt;- end + 1\n  }\n  return(-loglik)\n}\n# Estimate MLE\ninit_beta &lt;- rep(0, 4)\nresult &lt;- optim(\n  par = init_beta,\n  fn = neg_log_likelihood,\n  method = \"BFGS\",\n  hessian = TRUE\n)\n# Extract estimates\nbeta_hat &lt;- result$par\nhessian &lt;- result$hessian\nse &lt;- sqrt(diag(solve(hessian)))\nz &lt;- 1.96\nci_lower &lt;- beta_hat - z * se\nci_upper &lt;- beta_hat + z * se\n# Summarize results\nparam_names &lt;- c(\"β_netflix\", \"β_prime\", \"β_ads\", \"β_price\")\nsummary_df &lt;- data.frame(\n  Parameter = param_names,\n  Estimate = beta_hat,\n  Std.Error = se,\n  CI_Lower = ci_lower,\n  CI_Upper = ci_upper\n)\nprint(summary_df)\n\n\n  Parameter    Estimate   Std.Error   CI_Lower    CI_Upper\n1 β_netflix  0.94120473 0.111039639  0.7235670  1.15884242\n2   β_prime  0.50161701 0.111100000  0.2838610  0.71937301\n3     β_ads -0.73200143 0.087809687 -0.9041084 -0.55989445\n4   β_price -0.09948157 0.006333649 -0.1118955 -0.08706762"
  },
  {
    "objectID": "hw3_questions.html#estimation-via-bayesian-methods",
    "href": "hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nCode\n# Load data\ndf &lt;- read.csv(\"conjoint_data.csv\")\n# Create a choice set ID\ndf$resp_task &lt;- paste(df$resp, df$task, sep = \"_\")\n# Convert categorical variables to dummies (reference: brand H, ad No)\ndf$brand_N &lt;- ifelse(df$brand == \"N\", 1, 0)\ndf$brand_P &lt;- ifelse(df$brand == \"P\", 1, 0)\ndf$ad_Yes  &lt;- ifelse(df$ad == \"Yes\", 1, 0)\n# Build X matrix\nX &lt;- as.matrix(df[, c(\"brand_N\", \"brand_P\", \"ad_Yes\", \"price\")])\ny &lt;- df$choice\ngroup_sizes &lt;- table(df$resp_task)\n# Log-likelihood function\nlog_likelihood &lt;- function(beta) {\n  utilities &lt;- X %*% beta\n  start &lt;- 1\n  loglik &lt;- 0\n  for (size in group_sizes) {\n    end &lt;- start + size - 1\n    u_task &lt;- utilities[start:end]\n    y_task &lt;- y[start:end]\n    exp_u &lt;- exp(u_task - max(u_task))  # stability\n    probs &lt;- exp_u / sum(exp_u)\n    chosen_prob &lt;- probs[y_task == 1]\n    loglik &lt;- loglik + log(chosen_prob)\n    start &lt;- end + 1\n  }\n  return(loglik)\n}\n# Log-prior function\nlog_prior &lt;- function(beta) {\n  dnorm(beta[1], 0, sqrt(5), log = TRUE) +\n    dnorm(beta[2], 0, sqrt(5), log = TRUE) +\n    dnorm(beta[3], 0, sqrt(5), log = TRUE) +\n    dnorm(beta[4], 0, 1, log = TRUE)\n}\n# Log-posterior function\nlog_posterior &lt;- function(beta) {\n  log_likelihood(beta) + log_prior(beta)\n}\n# MCMC settings\nn_iter &lt;- 11000\nburn_in &lt;- 1000\nsamples &lt;- matrix(NA, nrow = n_iter, ncol = 4)\ncolnames(samples) &lt;- c(\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\")\n\n# Proposal standard deviations\nproposal_sd &lt;- c(0.05, 0.05, 0.05, 0.005)\n# Initialize\nset.seed(123)\nbeta_curr &lt;- rep(0, 4)\nlog_post_curr &lt;- log_posterior(beta_curr)\naccept &lt;- 0\n\n# MCMC loop\nfor (i in 1:n_iter) {\n  beta_prop &lt;- beta_curr + rnorm(4, 0, proposal_sd)\n  log_post_prop &lt;- log_posterior(beta_prop)\n  alpha &lt;- exp(log_post_prop - log_post_curr)\n  if (runif(1) &lt; alpha) {\n    beta_curr &lt;- beta_prop\n    log_post_curr &lt;- log_post_prop\n    accept &lt;- accept + 1\n  }\n  samples[i, ] &lt;- beta_curr\n}\n\naccept_rate &lt;- accept / n_iter\ncat(\"Acceptance Rate:\", round(accept_rate, 3), \"\\n\")\n\n\nAcceptance Rate: 0.564 \n\n\nCode\n# Drop burn-in\nposterior &lt;- samples[(burn_in + 1):n_iter, ]\n\n# Summary\nposterior_summary &lt;- apply(posterior, 2, function(x) {\n  c(mean = mean(x), sd = sd(x),\n    lower = quantile(x, 0.025),\n    upper = quantile(x, 0.975))\n})\nposterior_summary &lt;- as.data.frame(t(posterior_summary))\nposterior_summary\n\n\n\n\n\n\n\nmean\nsd\nlower.2.5%\nupper.97.5%\n\n\n\n\nbeta_netflix\n0.9257411\n0.1100357\n0.7219866\n1.1472743\n\n\nbeta_prime\n0.4882939\n0.1134613\n0.2674530\n0.7037160\n\n\nbeta_ads\n-0.7321228\n0.0887380\n-0.9103141\n-0.5551889\n\n\nbeta_price\n-0.0996789\n0.0062382\n-0.1122038\n-0.0874710\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1, 2))\n\n# Trace plot\nplot(posterior[, 4], type = \"l\", col = \"darkblue\",\n     main = \"Trace Plot: β_price\",\n     xlab = \"Iteration\", ylab = \"Value\")\n\n# Histogram of posterior distribution\nhist(posterior[, 4], breaks = 30, col = \"lightblue\", probability = TRUE,\n     main = \"Posterior Distribution: β_price\",\n     xlab = \"Value\", ylab = \"Density\")\nabline(v = mean(posterior[, 4]), col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n\nCode\n# Clean up posterior_summary from apply()\nposterior_summary &lt;- as.data.frame(t(posterior_summary))\nposterior_summary &lt;- posterior_summary[1:4, ]  # Only keep 4 rows\n\n# Add labels and structure\nposterior_summary$Method &lt;- \"Bayesian\"\nposterior_summary$Parameter &lt;- c(\"β_netflix\", \"β_prime\", \"β_ads\", \"β_price\")\n\n# Rename columns to match MLE summary\ncolnames(posterior_summary)[1:4] &lt;- c(\"Estimate\", \"Std.Error\", \"CI_Lower\", \"CI_Upper\")\nposterior_summary &lt;- posterior_summary[, c(\"Parameter\", \"Estimate\", \"Std.Error\", \"CI_Lower\", \"CI_Upper\", \"Method\")]\n\n# Ensure MLE summary matches format\nsummary_df$Method &lt;- \"MLE\"\nsummary_df &lt;- summary_df[, c(\"Parameter\", \"Estimate\", \"Std.Error\", \"CI_Lower\", \"CI_Upper\", \"Method\")]\n\n# Combine both tables\ncombined_results &lt;- rbind(summary_df, posterior_summary)\n\n# Display as a nice table\nknitr::kable(combined_results, digits = 4, caption = \"Comparison of MLE and Bayesian Estimates\")\n\n\n\nComparison of MLE and Bayesian Estimates\n\n\n\n\n\n\n\n\n\n\n\n\nParameter\nEstimate\nStd.Error\nCI_Lower\nCI_Upper\nMethod\n\n\n\n\n1\nβ_netflix\n0.9412\n0.1110\n0.7236\n1.1588\nMLE\n\n\n2\nβ_prime\n0.5016\n0.1111\n0.2839\n0.7194\nMLE\n\n\n3\nβ_ads\n-0.7320\n0.0878\n-0.9041\n-0.5599\nMLE\n\n\n4\nβ_price\n-0.0995\n0.0063\n-0.1119\n-0.0871\nMLE\n\n\nmean\nβ_netflix\n0.9257\n0.4883\n-0.7321\n-0.0997\nBayesian\n\n\nsd\nβ_prime\n0.1100\n0.1135\n0.0887\n0.0062\nBayesian\n\n\nlower.2.5%\nβ_ads\n0.7220\n0.2675\n-0.9103\n-0.1122\nBayesian\n\n\nupper.97.5%\nβ_price\n1.1473\n0.7037\n-0.5552\n-0.0875\nBayesian"
  },
  {
    "objectID": "hw3_questions.html#discussion",
    "href": "hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nSuppose we did not simulate the data and were analyzing results as if they came from a real-world survey. Here’s what we would observe from the parameter estimates:\n\n\\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\): This suggests that consumers, on average, prefer Netflix to Prime Video, holding ad presence and price constant. A higher coefficient for Netflix implies it provides higher utility or satisfaction relative to other brands.\n\\(\\beta_\\text{Prime} &gt; 0\\): While Netflix is preferred the most, the positive coefficient for Prime still indicates that it’s also more appealing than the baseline brand (Hulu). In practice, this aligns with brand equity and customer perception data for these platforms.\n\\(\\beta_\\text{price} &lt; 0\\): This is consistent with economic theory and intuition — as price increases, the likelihood of a product being chosen decreases. The negative coefficient confirms that consumers are price sensitive.\nPosterior vs MLE Results: Both methods yield very similar estimates, which is expected due to the relatively weak priors and sufficient data volume. This confirms the robustness of the MLE results and shows how Bayesian methods can reinforce frequentist inference.\n\nIn summary, the parameter estimates are sensible, interpretable, and align with expected consumer behavior. Even without knowing the underlying data was simulated, the model provides valuable insights into preferences for brand, price sensitivity, and advertising tolerance in streaming service choices.\n\nExtension to Hierarchical (Multilevel) Models\nTo move from a basic Multinomial Logit (MNL) model to a hierarchical or random-parameters logit model — which is more common in real-world conjoint analysis — the key idea is to allow individual-level heterogeneity in preferences.\n\nConceptual Change\nIn the standard MNL model, we assume: \\[\n\\beta_i = \\beta \\quad \\text{(same for all individuals)}\n\\]\nIn a hierarchical model, we instead assume: \\[\n\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\n\\]\nEach respondent ( i ) has their own set of parameters ( _i ), drawn from a population-level distribution with mean ( ) and covariance ( ).\n\n\nSimulation Changes\nTo simulate data from a hierarchical model: - Draw a different ( _i ) for each respondent from a common prior (e.g., multivariate normal) - Use these individual ( _i )s to compute utility and choices across tasks - Store and analyze the data accordingly\n\n\nEstimation Changes\nTo estimate a hierarchical model: - Use Bayesian methods such as Hierarchical Bayes (HB) via MCMC, or frequentist mixed logit (e.g., using mlogit or bayesm in R) - You’ll now estimate: - The mean vector ( ) of the population - The covariance matrix ( ) (to capture variation in preferences) - And each individual’s ( _i )\n\n\nWhy This Matters\n\nReal-world consumers do not all have the same preferences.\nHierarchical models better capture preference heterogeneity.\nThis leads to more accurate predictions, better market segmentation, and more realistic simulations of behavior."
  },
  {
    "objectID": "hw3_questions.html#discussion-1",
    "href": "hw3_questions.html#discussion-1",
    "title": "Multinomial Logit Model",
    "section": "7 6. Discussion",
    "text": "7 6. Discussion\nSuppose we did not simulate the data and were analyzing results as if they came from a real-world survey. Here’s what we would observe from the parameter estimates:\n\n\\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\): This suggests that consumers, on average, prefer Netflix to Prime Video, holding ad presence and price constant. A higher coefficient for Netflix implies it provides higher utility or satisfaction relative to other brands.\n\\(\\beta_\\text{Prime} &gt; 0\\): While Netflix is preferred the most, the positive coefficient for Prime still indicates that it’s also more appealing than the baseline brand (Hulu). In practice, this aligns with brand equity and customer perception data for these platforms.\n\\(\\beta_\\text{price} &lt; 0\\): This is consistent with economic theory and intuition — as price increases, the likelihood of a product being chosen decreases. The negative coefficient confirms that consumers are price sensitive.\nPosterior vs MLE Results: Both methods yield very similar estimates, which is expected due to the relatively weak priors and sufficient data volume. This confirms the robustness of the MLE results and shows how Bayesian methods can reinforce frequentist inference.\n\nIn summary, the parameter estimates are sensible, interpretable, and align with expected consumer behavior. Even without knowing the underlying data was simulated, the model provides valuable insights into preferences for brand, price sensitivity, and advertising tolerance in streaming service choices.\ntodo: At a high level, discuss what change you would need to make in order to simulate data from — and estimate the parameters of — a multi-level (aka random-parameter or hierarchical) model. This is the model we use to analyze “real world” conjoint data.\n\n7.1 Extension to Hierarchical (Multilevel) Models\nTo move from a basic Multinomial Logit (MNL) model to a hierarchical or random-parameters logit model — which is more common in real-world conjoint analysis — the key idea is to allow individual-level heterogeneity in preferences.\n\n7.1.1 🧠 Conceptual Change\nIn the standard MNL model, we assume: \\[\n\\beta_i = \\beta \\quad \\text{(same for all individuals)}\n\\]\nIn a hierarchical model, we instead assume: \\[\n\\beta_i \\sim \\mathcal{N}(\\mu, \\Sigma)\n\\]\nEach respondent ( i ) has their own set of parameters ( _i ), drawn from a population-level distribution with mean ( ) and covariance ( ).\n\n\n7.1.2 🔧 Simulation Changes\nTo simulate data from a hierarchical model: - Draw a different ( _i ) for each respondent from a common prior (e.g., multivariate normal) - Use these individual ( _i )s to compute utility and choices across tasks - Store and analyze the data accordingly\n\n\n7.1.3 🛠 Estimation Changes\nTo estimate a hierarchical model: - Use Bayesian methods such as Hierarchical Bayes (HB) via MCMC, or frequentist mixed logit (e.g., using mlogit or bayesm in R) - You’ll now estimate: - The mean vector ( ) of the population - The covariance matrix ( ) (to capture variation in preferences) - And each individual’s ( _i )\n\n\n7.1.4 📈 Why This Matters\n\nReal-world consumers do not all have the same preferences.\nHierarchical models better capture preference heterogeneity.\nThis leads to more accurate predictions, better market segmentation, and more realistic simulations of behavior."
  },
  {
    "objectID": "hw4_questions.html",
    "href": "hw4_questions.html",
    "title": "Key Drivers Analysis",
    "section": "",
    "text": "This post implements a few measure of variable importance, interpreted as a key drivers analysis, for certain aspects of a payment card on customer satisfaction with that payment card.\ntodo: replicate the table on slide 19 of the session 4 slides. This involves calculating pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python.\nIf you want a challenge, either (1) implement one or more of the measures yourself. “Usefulness” is rather easy to program up. Shapley values for linear regression are a bit more work. Or (2) add additional measures to the table such as the importance scores from XGBoost."
  },
  {
    "objectID": "hw4_questions (1).html",
    "href": "hw4_questions (1).html",
    "title": "K-means",
    "section": "",
    "text": "# Use base R to read the CSV file (no tidyverse required)\npenguins &lt;- read.csv(\"palmer_penguins.csv\", stringsAsFactors = TRUE)\nhead(penguins)\n\n  species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1  Adelie Torgersen           39.1          18.7               181        3750\n2  Adelie Torgersen           39.5          17.4               186        3800\n3  Adelie Torgersen           40.3          18.0               195        3250\n4  Adelie Torgersen           36.7          19.3               193        3450\n5  Adelie Torgersen           39.3          20.6               190        3650\n6  Adelie Torgersen           38.9          17.8               181        3625\n     sex year\n1   male 2007\n2 female 2007\n3 female 2007\n4 female 2007\n5   male 2007\n6 female 2007\n\n\n\n\n\n\npenguins_filtered &lt;- penguins[, c(\"bill_length_mm\", \"flipper_length_mm\")]\npenguins_filtered &lt;- penguins_filtered[complete.cases(penguins_filtered), ]\ncat(\"Number of rows after dropping NAs:\", nrow(penguins_filtered), \"\\n\")\n\nNumber of rows after dropping NAs: 333 \n\npenguins_filtered[sample(nrow(penguins_filtered), 5), ]\n\n    bill_length_mm flipper_length_mm\n178           45.1               207\n255           49.8               229\n213           43.5               220\n124           44.1               210\n218           46.5               217\n\n\n\n\n\n\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\n\nLoading required package: ggplot2\n\nlibrary(ggplot2)\n\nggplot(penguins_filtered, aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point(color = \"steelblue\", alpha = 0.7, size = 2) +\n  labs(\n    title = \"Scatter Plot of Bill Length vs Flipper Length\",\n    x = \"Bill Length (mm)\",\n    y = \"Flipper Length (mm)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\neuclidean_dist &lt;- function(a, b) {\n  sqrt(rowSums((a - b)^2))\n}\n\n\n\n\n\nset.seed(42)\ndata_matrix &lt;- as.matrix(penguins_filtered)\nK &lt;- 3\ninitial_indices &lt;- sample(1:nrow(data_matrix), K)\ncentroids &lt;- data_matrix[initial_indices, ]\nprint(centroids)\n\n    bill_length_mm flipper_length_mm\n49            34.5               187\n321           52.2               197\n153           45.4               211\n\n\n\n\n\n\nmax_iter &lt;- 10\nclusters &lt;- rep(0, nrow(data_matrix))\n\nfor (iter in 1:max_iter) {\n  for (i in 1:nrow(data_matrix)) {\n    distances &lt;- apply(centroids, 1, function(centroid) {\n      sum((data_matrix[i, ] - centroid)^2)\n    })\n    clusters[i] &lt;- which.min(distances)\n  }\n\n  new_centroids &lt;- centroids\n  for (k in 1:K) {\n    cluster_points &lt;- data_matrix[clusters == k, ]\n    if (nrow(cluster_points) &gt; 0) {\n      new_centroids[k, ] &lt;- colMeans(cluster_points)\n    }\n  }\n\n  cluster_df &lt;- as.data.frame(data_matrix)\n  cluster_df$cluster &lt;- as.factor(clusters)\n  centroid_df &lt;- as.data.frame(new_centroids)\n  colnames(centroid_df) &lt;- c(\"bill_length_mm\", \"flipper_length_mm\")\n\n  p &lt;- ggplot(cluster_df, aes(x = bill_length_mm, y = flipper_length_mm, color = cluster)) +\n    geom_point(size = 2, alpha = 0.7) +\n    geom_point(data = centroid_df, aes(x = bill_length_mm, y = flipper_length_mm),\n               color = \"black\", size = 5, shape = 8) +\n    scale_color_manual(values = c(\"#E69F00\", \"#56B4E9\", \"#009E73\")) +\n    labs(title = paste(\"K-Means Iteration\", iter),\n         x = \"Bill Length (mm)\",\n         y = \"Flipper Length (mm)\") +\n    theme_minimal(base_size = 14) +\n    theme(panel.grid.major = element_line(color = \"grey90\"),\n          legend.position = \"none\")\n\n  print(p)\n\n  ggsave(filename = sprintf(\"kmeans_iter_%02d.png\", iter),\n         plot = p + theme(plot.background = element_rect(fill = \"white\", color = NA)),\n         width = 6, height = 5, dpi = 150)\n\n  if (all(new_centroids == centroids)) {\n    cat(\"Converged at iteration\", iter, \"\\n\")\n    break\n  }\n  centroids &lt;- new_centroids\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConverged at iteration 6 \n\n\n\n\n\n\nif (!require(\"cluster\")) install.packages(\"cluster\")\n\nLoading required package: cluster\n\nif (!require(\"factoextra\")) install.packages(\"factoextra\")\n\nLoading required package: factoextra\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nlibrary(cluster)\nlibrary(factoextra)\n\nwcss &lt;- numeric()\nsilhouette_scores &lt;- numeric()\n\nfor (k in 2:7) {\n  set.seed(42)\n  kmeans_model &lt;- kmeans(data_matrix, centers = k, nstart = 10)\n  wcss[k] &lt;- sum(kmeans_model$withinss)\n  sil &lt;- silhouette(kmeans_model$cluster, dist(data_matrix))\n  silhouette_scores[k] &lt;- mean(sil[, 3])\n}\n\nwcss_df &lt;- data.frame(K = 2:7, WCSS = wcss[2:7])\nsil_df &lt;- data.frame(K = 2:7, Silhouette = silhouette_scores[2:7])\n\n\n\n\n\nggplot(wcss_df, aes(x = K, y = WCSS)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  geom_point(size = 3) +\n  labs(title = \"Elbow Method: WCSS vs K\", x = \"Number of Clusters (K)\", y = \"WCSS\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(sil_df, aes(x = K, y = Silhouette)) +\n  geom_line(color = \"darkgreen\", size = 1.2) +\n  geom_point(size = 3) +\n  labs(title = \"Silhouette Scores vs K\", x = \"Number of Clusters (K)\", y = \"Silhouette\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plot showed a sharp drop from K = 2 to K = 3, after which the reduction in WCSS slowed.\nThis indicates an elbow at K = 3, which often suggests that K = 3 captures the main structure in the data.\n\n\n\nThe highest average silhouette score was observed at K = 2, indicating that the data points are more clearly and tightly grouped when divided into 2 clusters.\nSilhouette scores typically favor compact and well-separated clusters.\n\n\n\nMetric Suggested K Elbow Method 3 Silhouette Score 2\nTherefore, K = 2 is recommended as the “right” number of clusters, because it yields the best separation and tightest clusters according to the silhouette score, which is a more interpretable metric for cluster quality."
  },
  {
    "objectID": "hw4_questions (1).html#a.-k-means",
    "href": "hw4_questions (1).html#a.-k-means",
    "title": "K-means",
    "section": "",
    "text": "# Use base R to read the CSV file (no tidyverse required)\npenguins &lt;- read.csv(\"palmer_penguins.csv\", stringsAsFactors = TRUE)\nhead(penguins)\n\n  species    island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n1  Adelie Torgersen           39.1          18.7               181        3750\n2  Adelie Torgersen           39.5          17.4               186        3800\n3  Adelie Torgersen           40.3          18.0               195        3250\n4  Adelie Torgersen           36.7          19.3               193        3450\n5  Adelie Torgersen           39.3          20.6               190        3650\n6  Adelie Torgersen           38.9          17.8               181        3625\n     sex year\n1   male 2007\n2 female 2007\n3 female 2007\n4 female 2007\n5   male 2007\n6 female 2007\n\n\n\n\n\n\npenguins_filtered &lt;- penguins[, c(\"bill_length_mm\", \"flipper_length_mm\")]\npenguins_filtered &lt;- penguins_filtered[complete.cases(penguins_filtered), ]\ncat(\"Number of rows after dropping NAs:\", nrow(penguins_filtered), \"\\n\")\n\nNumber of rows after dropping NAs: 333 \n\npenguins_filtered[sample(nrow(penguins_filtered), 5), ]\n\n    bill_length_mm flipper_length_mm\n178           45.1               207\n255           49.8               229\n213           43.5               220\n124           44.1               210\n218           46.5               217\n\n\n\n\n\n\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\n\nLoading required package: ggplot2\n\nlibrary(ggplot2)\n\nggplot(penguins_filtered, aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point(color = \"steelblue\", alpha = 0.7, size = 2) +\n  labs(\n    title = \"Scatter Plot of Bill Length vs Flipper Length\",\n    x = \"Bill Length (mm)\",\n    y = \"Flipper Length (mm)\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\neuclidean_dist &lt;- function(a, b) {\n  sqrt(rowSums((a - b)^2))\n}\n\n\n\n\n\nset.seed(42)\ndata_matrix &lt;- as.matrix(penguins_filtered)\nK &lt;- 3\ninitial_indices &lt;- sample(1:nrow(data_matrix), K)\ncentroids &lt;- data_matrix[initial_indices, ]\nprint(centroids)\n\n    bill_length_mm flipper_length_mm\n49            34.5               187\n321           52.2               197\n153           45.4               211\n\n\n\n\n\n\nmax_iter &lt;- 10\nclusters &lt;- rep(0, nrow(data_matrix))\n\nfor (iter in 1:max_iter) {\n  for (i in 1:nrow(data_matrix)) {\n    distances &lt;- apply(centroids, 1, function(centroid) {\n      sum((data_matrix[i, ] - centroid)^2)\n    })\n    clusters[i] &lt;- which.min(distances)\n  }\n\n  new_centroids &lt;- centroids\n  for (k in 1:K) {\n    cluster_points &lt;- data_matrix[clusters == k, ]\n    if (nrow(cluster_points) &gt; 0) {\n      new_centroids[k, ] &lt;- colMeans(cluster_points)\n    }\n  }\n\n  cluster_df &lt;- as.data.frame(data_matrix)\n  cluster_df$cluster &lt;- as.factor(clusters)\n  centroid_df &lt;- as.data.frame(new_centroids)\n  colnames(centroid_df) &lt;- c(\"bill_length_mm\", \"flipper_length_mm\")\n\n  p &lt;- ggplot(cluster_df, aes(x = bill_length_mm, y = flipper_length_mm, color = cluster)) +\n    geom_point(size = 2, alpha = 0.7) +\n    geom_point(data = centroid_df, aes(x = bill_length_mm, y = flipper_length_mm),\n               color = \"black\", size = 5, shape = 8) +\n    scale_color_manual(values = c(\"#E69F00\", \"#56B4E9\", \"#009E73\")) +\n    labs(title = paste(\"K-Means Iteration\", iter),\n         x = \"Bill Length (mm)\",\n         y = \"Flipper Length (mm)\") +\n    theme_minimal(base_size = 14) +\n    theme(panel.grid.major = element_line(color = \"grey90\"),\n          legend.position = \"none\")\n\n  print(p)\n\n  ggsave(filename = sprintf(\"kmeans_iter_%02d.png\", iter),\n         plot = p + theme(plot.background = element_rect(fill = \"white\", color = NA)),\n         width = 6, height = 5, dpi = 150)\n\n  if (all(new_centroids == centroids)) {\n    cat(\"Converged at iteration\", iter, \"\\n\")\n    break\n  }\n  centroids &lt;- new_centroids\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConverged at iteration 6 \n\n\n\n\n\n\nif (!require(\"cluster\")) install.packages(\"cluster\")\n\nLoading required package: cluster\n\nif (!require(\"factoextra\")) install.packages(\"factoextra\")\n\nLoading required package: factoextra\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nlibrary(cluster)\nlibrary(factoextra)\n\nwcss &lt;- numeric()\nsilhouette_scores &lt;- numeric()\n\nfor (k in 2:7) {\n  set.seed(42)\n  kmeans_model &lt;- kmeans(data_matrix, centers = k, nstart = 10)\n  wcss[k] &lt;- sum(kmeans_model$withinss)\n  sil &lt;- silhouette(kmeans_model$cluster, dist(data_matrix))\n  silhouette_scores[k] &lt;- mean(sil[, 3])\n}\n\nwcss_df &lt;- data.frame(K = 2:7, WCSS = wcss[2:7])\nsil_df &lt;- data.frame(K = 2:7, Silhouette = silhouette_scores[2:7])\n\n\n\n\n\nggplot(wcss_df, aes(x = K, y = WCSS)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  geom_point(size = 3) +\n  labs(title = \"Elbow Method: WCSS vs K\", x = \"Number of Clusters (K)\", y = \"WCSS\") +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n\n\n\n\nggplot(sil_df, aes(x = K, y = Silhouette)) +\n  geom_line(color = \"darkgreen\", size = 1.2) +\n  geom_point(size = 3) +\n  labs(title = \"Silhouette Scores vs K\", x = \"Number of Clusters (K)\", y = \"Silhouette\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe plot showed a sharp drop from K = 2 to K = 3, after which the reduction in WCSS slowed.\nThis indicates an elbow at K = 3, which often suggests that K = 3 captures the main structure in the data.\n\n\n\nThe highest average silhouette score was observed at K = 2, indicating that the data points are more clearly and tightly grouped when divided into 2 clusters.\nSilhouette scores typically favor compact and well-separated clusters.\n\n\n\nMetric Suggested K Elbow Method 3 Silhouette Score 2\nTherefore, K = 2 is recommended as the “right” number of clusters, because it yields the best separation and tightest clusters according to the silhouette score, which is a more interpretable metric for cluster quality."
  },
  {
    "objectID": "hw4_questions (1).html#step-1-load-and-prepare-the-data-from-csv-file",
    "href": "hw4_questions (1).html#step-1-load-and-prepare-the-data-from-csv-file",
    "title": "Add Title",
    "section": "Step 1: Load and Prepare the Data (from CSV file)",
    "text": "Step 1: Load and Prepare the Data (from CSV file)\n\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\nLoading required package: tidyverse\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(tidyverse)\n\n# Read the CSV file\npenguins &lt;- read_csv(\"palmer_penguins.csv\")\n\nRows: 333 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (3): species, island, sex\ndbl (5): bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g, year\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Preview the dataset\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;chr&gt;   &lt;chr&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           36.7          19.3               193        3450\n5 Adelie  Torgersen           39.3          20.6               190        3650\n6 Adelie  Torgersen           38.9          17.8               181        3625\n# ℹ 2 more variables: sex &lt;chr&gt;, year &lt;dbl&gt;"
  },
  {
    "objectID": "hw4_questions (1).html#step-1.1-select-required-columns-and-drop-missing-values",
    "href": "hw4_questions (1).html#step-1.1-select-required-columns-and-drop-missing-values",
    "title": "Add Title",
    "section": "Step 1.1: Select Required Columns and Drop Missing Values",
    "text": "Step 1.1: Select Required Columns and Drop Missing Values\n\npenguins_filtered &lt;- penguins %&gt;%\n  select(bill_length_mm, flipper_length_mm) %&gt;%\n  drop_na()\n\ncat(\"Number of rows after dropping NAs:\", nrow(penguins_filtered), \"\\n\")\n\nNumber of rows after dropping NAs: 333 \n\nsample_n(penguins_filtered, 5)\n\n# A tibble: 5 × 2\n  bill_length_mm flipper_length_mm\n           &lt;dbl&gt;             &lt;dbl&gt;\n1           35.3               187\n2           43.2               187\n3           42.5               187\n4           40.6               187\n5           46.7               195"
  },
  {
    "objectID": "hw4_questions (1).html#step-1.2-visualize-the-data",
    "href": "hw4_questions (1).html#step-1.2-visualize-the-data",
    "title": "Add Title",
    "section": "Step 1.2: Visualize the Data",
    "text": "Step 1.2: Visualize the Data\n\nggplot(penguins_filtered, aes(x = bill_length_mm, y = flipper_length_mm)) +\n  geom_point(color = \"steelblue\", alpha = 0.7, size = 2) +\n  labs(\n    title = \"Scatter Plot of Bill Length vs Flipper Length\",\n    x = \"Bill Length (mm)\",\n    y = \"Flipper Length (mm)\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "hw4_questions (1).html#step-2-k-means-clustering-from-scratch-in-r",
    "href": "hw4_questions (1).html#step-2-k-means-clustering-from-scratch-in-r",
    "title": "Add Title",
    "section": "Step 2: K-Means Clustering from Scratch in R",
    "text": "Step 2: K-Means Clustering from Scratch in R\n\neuclidean_dist &lt;- function(a, b) {\n  sqrt(rowSums((a - b)^2))\n}"
  },
  {
    "objectID": "hw4_questions (1).html#step-2.1-initialize-centroids",
    "href": "hw4_questions (1).html#step-2.1-initialize-centroids",
    "title": "Add Title",
    "section": "Step 2.1: Initialize Centroids",
    "text": "Step 2.1: Initialize Centroids\n\nset.seed(42)\ndata_matrix &lt;- as.matrix(penguins_filtered)\nK &lt;- 3\ninitial_indices &lt;- sample(1:nrow(data_matrix), K)\ncentroids &lt;- data_matrix[initial_indices, ]\nprint(centroids)\n\n     bill_length_mm flipper_length_mm\n[1,]           34.5               187\n[2,]           52.2               197\n[3,]           45.4               211"
  },
  {
    "objectID": "hw4_questions (1).html#step-2.2-run-k-means-algorithm-manually-with-image-saving",
    "href": "hw4_questions (1).html#step-2.2-run-k-means-algorithm-manually-with-image-saving",
    "title": "Add Title",
    "section": "Step 2.2: Run K-Means Algorithm Manually with Image Saving",
    "text": "Step 2.2: Run K-Means Algorithm Manually with Image Saving\n\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\nlibrary(ggplot2)\n\nmax_iter &lt;- 10\nclusters &lt;- rep(0, nrow(data_matrix))\n\nfor (iter in 1:max_iter) {\n  for (i in 1:nrow(data_matrix)) {\n    distances &lt;- apply(centroids, 1, function(centroid) {\n      sum((data_matrix[i, ] - centroid)^2)\n    })\n    clusters[i] &lt;- which.min(distances)\n  }\n\n  new_centroids &lt;- centroids\n  for (k in 1:K) {\n    cluster_points &lt;- data_matrix[clusters == k, ]\n    if (nrow(cluster_points) &gt; 0) {\n      new_centroids[k, ] &lt;- colMeans(cluster_points)\n    }\n  }\n\n  cluster_df &lt;- as.data.frame(data_matrix)\n  cluster_df$cluster &lt;- as.factor(clusters)\n\n  centroid_df &lt;- as.data.frame(new_centroids)\n  colnames(centroid_df) &lt;- c(\"bill_length_mm\", \"flipper_length_mm\")\n  p &lt;- ggplot(cluster_df, aes(x = bill_length_mm, y = flipper_length_mm, color = cluster)) +\n  geom_point(size = 2, alpha = 0.7) +\n  geom_point(data = centroid_df, aes(x = bill_length_mm, y = flipper_length_mm),\n             color = \"Black\", size = 5, shape = 8) +  # star shape for centroids\n  scale_color_manual(values = c(\"#E69F00\", \"#56B4E9\", \"#009E73\")) +  # orange, blue, green\n  labs(\n    title = paste(\"K-Means Iteration\", iter),\n    x = \"Bill Length (mm)\",\n    y = \"Flipper Length (mm)\"\n  ) +\n  theme_minimal(base_size = 14) +\n  theme(\n    panel.grid.major = element_line(color = \"grey90\"),\n    legend.position = \"none\"\n  )\n  print(p)\n\n  ggsave(\n  filename = sprintf(\"kmeans_iter_%02d.png\", iter),\n  plot = p + theme(plot.background = element_rect(fill = \"white\", color = NA)),\n  width = 6,\n  height = 5,\n  dpi = 150\n)\n\n  if (all(new_centroids == centroids)) {\n    cat(\"Converged at iteration\", iter, \"\\n\")\n    break\n  }\n\n  centroids &lt;- new_centroids\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConverged at iteration 6"
  },
  {
    "objectID": "hw4_questions (1).html#step-3-compare-with-rs-built-in-kmeans-and-evaluate-clusters",
    "href": "hw4_questions (1).html#step-3-compare-with-rs-built-in-kmeans-and-evaluate-clusters",
    "title": "Add Title",
    "section": "Step 3: Compare with R’s Built-in kmeans() and Evaluate Clusters",
    "text": "Step 3: Compare with R’s Built-in kmeans() and Evaluate Clusters\n\nif (!require(\"cluster\")) install.packages(\"cluster\")\n\nLoading required package: cluster\n\nif (!require(\"factoextra\")) install.packages(\"factoextra\")\n\nLoading required package: factoextra\n\n\nWelcome! Want to learn more? See two factoextra-related books at https://goo.gl/ve3WBa\n\nlibrary(cluster)\nlibrary(factoextra)"
  },
  {
    "objectID": "hw4_questions (1).html#step-3.1-compute-wcss-and-silhouette-scores-for-k-2-to-7",
    "href": "hw4_questions (1).html#step-3.1-compute-wcss-and-silhouette-scores-for-k-2-to-7",
    "title": "Add Title",
    "section": "Step 3.1: Compute WCSS and Silhouette Scores for K = 2 to 7",
    "text": "Step 3.1: Compute WCSS and Silhouette Scores for K = 2 to 7\n\ndata_matrix &lt;- penguins_filtered %&gt;% select(bill_length_mm, flipper_length_mm)\nwcss &lt;- numeric()\nsilhouette_scores &lt;- numeric()\n\nfor (k in 2:7) {\n  set.seed(42)\n  kmeans_model &lt;- kmeans(data_matrix, centers = k, nstart = 10)\n  wcss[k] &lt;- sum(kmeans_model$withinss)\n  sil &lt;- silhouette(kmeans_model$cluster, dist(data_matrix))\n  silhouette_scores[k] &lt;- mean(sil[, 3])\n}"
  },
  {
    "objectID": "hw4_questions (1).html#step-3.2-plot-elbow-curve-wcss",
    "href": "hw4_questions (1).html#step-3.2-plot-elbow-curve-wcss",
    "title": "Add Title",
    "section": "Step 3.2: Plot Elbow Curve (WCSS)",
    "text": "Step 3.2: Plot Elbow Curve (WCSS)\n\nwcss_df &lt;- data.frame(K = 2:7, WCSS = wcss[2:7])\n\nggplot(wcss_df, aes(x = K, y = WCSS)) +\n  geom_line(color = \"steelblue\", size = 1.2) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Elbow Method: WCSS vs K\",\n    x = \"Number of Clusters (K)\",\n    y = \"Within-Cluster Sum of Squares (WCSS)\"\n  ) +\n  theme_minimal()\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead."
  },
  {
    "objectID": "hw4_questions (1).html#step-3.3-plot-silhouette-scores",
    "href": "hw4_questions (1).html#step-3.3-plot-silhouette-scores",
    "title": "Add Title",
    "section": "Step 3.3: Plot Silhouette Scores",
    "text": "Step 3.3: Plot Silhouette Scores\n\nsil_df &lt;- data.frame(K = 2:7, Silhouette = silhouette_scores[2:7])\n\nggplot(sil_df, aes(x = K, y = Silhouette)) +\n  geom_line(color = \"darkgreen\", size = 1.2) +\n  geom_point(size = 3) +\n  labs(\n    title = \"Silhouette Scores vs K\",\n    x = \"Number of Clusters (K)\",\n    y = \"Average Silhouette Score\"\n  ) +\n  theme_minimal()"
  },
  {
    "objectID": "hw4_questions (1).html#step-4-visualize-clusters-for-k-2-and-k-3-using-fviz_cluster",
    "href": "hw4_questions (1).html#step-4-visualize-clusters-for-k-2-and-k-3-using-fviz_cluster",
    "title": "Add Title",
    "section": "Step 4: Visualize Clusters for K = 2 and K = 3 Using fviz_cluster()",
    "text": "Step 4: Visualize Clusters for K = 2 and K = 3 Using fviz_cluster()\n\ndata_matrix &lt;- penguins_filtered %&gt;% select(bill_length_mm, flipper_length_mm)\n\nset.seed(42)\nk2 &lt;- kmeans(data_matrix, centers = 2, nstart = 25)\n\nfviz_cluster(k2, data = data_matrix,\n             geom = \"point\",\n             ellipse.type = \"norm\",\n             palette = \"Set2\",\n             main = \"K-means Clustering (K = 2)\")\n\n\n\n\n\n\n\n\n\nset.seed(42)\nk3 &lt;- kmeans(data_matrix, centers = 3, nstart = 25)\n\nfviz_cluster(k3, data = data_matrix,\n             geom = \"point\",\n             ellipse.type = \"norm\",\n             palette = \"Set1\",\n             main = \"K-means Clustering (K = 3)\")"
  },
  {
    "objectID": "hw4_questions (1).html#step-5-determine-the-optimal-number-of-clusters-k",
    "href": "hw4_questions (1).html#step-5-determine-the-optimal-number-of-clusters-k",
    "title": "Add Title",
    "section": "Step 5: Determine the Optimal Number of Clusters (K)",
    "text": "Step 5: Determine the Optimal Number of Clusters (K)\nTo determine the most appropriate number of clusters, we used two widely accepted methods:\n\n1. Elbow Method (WCSS)\n\nShows sharp decrease from K = 2 to K = 3, suggesting elbow at K = 3.\n\n\n\n2. Silhouette Score\n\nPeak at K = 2, suggesting better-defined clusters.\n\n\n\nFinal Recommendation\n\n\n\nMetric\nBest K\n\n\n\n\nElbow (WCSS)\n3\n\n\nSilhouette Score\n2\n\n\n\n\nRecommendation: Choose K = 2 for highest silhouette score."
  },
  {
    "objectID": "hw4_questions (1).html#step-6-create-an-animated-gif-of-k-means-iterations",
    "href": "hw4_questions (1).html#step-6-create-an-animated-gif-of-k-means-iterations",
    "title": "Add Title",
    "section": "Step 6: Create an Animated GIF of K-Means Iterations",
    "text": "Step 6: Create an Animated GIF of K-Means Iterations\n\nif (!require(\"magick\")) install.packages(\"magick\")\n\nLoading required package: magick\n\n\nLinking to ImageMagick 6.9.12.93\nEnabled features: cairo, fontconfig, freetype, heic, lcms, pango, raw, rsvg, webp\nDisabled features: fftw, ghostscript, x11\n\nlibrary(magick)\n\nimgs &lt;- list.files(pattern = \"kmeans_iter_\\\\d+\\\\.png\")\nframes &lt;- image_read(imgs)\nanimation &lt;- image_animate(frames, fps = 1)\nimage_write(animation, \"kmeans_animation.gif\")"
  },
  {
    "objectID": "hw4_questions (1).html#step-7-embed-the-gif",
    "href": "hw4_questions (1).html#step-7-embed-the-gif",
    "title": "Add Title",
    "section": "Step 7: Embed the GIF",
    "text": "Step 7: Embed the GIF\n\n![K-means Clustering Animation](kmeans_animation.gif)"
  },
  {
    "objectID": "hw4_questions (1).html#b.-latent-class-mnl",
    "href": "hw4_questions (1).html#b.-latent-class-mnl",
    "title": "Add Title",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current “wide” format into a “long” format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate – akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "hw4_questions (1).html#a.-k-nearest-neighbors",
    "href": "hw4_questions (1).html#a.-k-nearest-neighbors",
    "title": "K-means",
    "section": "2a. K-Nearest Neighbors",
    "text": "2a. K-Nearest Neighbors\n\nGenerate Data\n\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nboundary &lt;- sin(4 * x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\n\n\nPlot with Boundary\n\nx1_seq &lt;- seq(-3, 3, length.out = 300)\nboundary_curve &lt;- sin(4 * x1_seq) + x1_seq\nboundary_df &lt;- data.frame(x1 = x1_seq, x2 = boundary_curve)\n\nggplot(dat, aes(x = x1, y = x2, color = y)) +\n  geom_point(size = 2, alpha = 0.8) +\n  geom_line(data = boundary_df, aes(x = x1, y = x2), color = \"black\") +\n  scale_color_manual(values = c(\"#E69F00\", \"#56B4E9\")) +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\nGenerate Test Set\n\nset.seed(99)\nn_test &lt;- 100\nx1_test &lt;- runif(n_test, -3, 3)\nx2_test &lt;- runif(n_test, -3, 3)\nboundary_test &lt;- sin(4 * x1_test) + x1_test\ny_test &lt;- ifelse(x2_test &gt; boundary_test, 1, 0) |&gt; as.factor()\ntest_dat &lt;- data.frame(x1 = x1_test, x2 = x2_test, y = y_test)\n\n\n\nKNN Function\n\neuclidean_distance &lt;- function(a, b) {\n  sqrt(sum((a - b)^2))\n}\n\nknn_predict &lt;- function(train_x, train_y, test_x, k = 3) {\n  predictions &lt;- vector(length = nrow(test_x))\n  for (i in 1:nrow(test_x)) {\n    distances &lt;- apply(train_x, 1, function(row) euclidean_distance(row, test_x[i, ]))\n    nearest_idx &lt;- order(distances)[1:k]\n    nearest_labels &lt;- train_y[nearest_idx]\n    predictions[i] &lt;- names(sort(table(nearest_labels), decreasing = TRUE))[1]\n  }\n  return(as.factor(predictions))\n}\n\n\n\nEvaluate for k = 1 to 30\n\ntrain_x &lt;- dat[, c(\"x1\", \"x2\")]\ntrain_y &lt;- dat$y\ntest_x &lt;- test_dat[, c(\"x1\", \"x2\")]\ntest_y &lt;- test_dat$y\n\naccuracy_results &lt;- numeric(30)\nfor (k in 1:30) {\n  preds &lt;- knn_predict(train_x, train_y, test_x, k)\n  accuracy_results[k] &lt;- mean(preds == test_y) * 100\n}\n\naccuracy_df &lt;- data.frame(k = 1:30, accuracy = accuracy_results)\nbest_k &lt;- which.max(accuracy_results)\nbest_accuracy &lt;- accuracy_results[best_k]\n\ncat(\"✅ Optimal k:\", best_k, \"\\n📈 Highest accuracy:\", round(best_accuracy, 2), \"%\\n\")\n\n✅ Optimal k: 18 \n📈 Highest accuracy: 88 %\n\n\n\n\nPlot Accuracy vs K\n\nggplot(accuracy_df, aes(x = k, y = accuracy)) +\n  geom_line(color = \"steelblue\", linewidth = 1.2) +\n  geom_point(size = 2) +\n  geom_vline(xintercept = best_k, linetype = \"dashed\", color = \"red\") +\n  labs(\n    title = \"KNN Classification Accuracy vs K\",\n    x = \"Number of Neighbors (k)\",\n    y = \"Accuracy (%)\"\n  ) +\n  theme_minimal(base_size = 14)"
  },
  {
    "objectID": "hw4_questions (1).html#b.-key-drivers-analysis",
    "href": "hw4_questions (1).html#b.-key-drivers-analysis",
    "title": "Add Title",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  },
  {
    "objectID": "hw4_questions (2).html",
    "href": "hw4_questions (2).html",
    "title": "Add Title",
    "section": "",
    "text": "todo: do two analyses. Do one of either 1a or 1b, AND one of either 2a or 2b."
  },
  {
    "objectID": "hw4_questions (2).html#a.-k-means",
    "href": "hw4_questions (2).html#a.-k-means",
    "title": "Add Title",
    "section": "1a. K-Means",
    "text": "1a. K-Means\ntodo: write your own code to implement the k-means algorithm. Make plots of the various steps the algorithm takes so you can “see” the algorithm working. Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables. Compare your results to the built-in kmeans function in R or Python.\ntodo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,…,7). What is the “right” number of clusters as suggested by these two metrics?\nIf you want a challenge, add your plots as an animated gif on your website so that the result looks something like this."
  },
  {
    "objectID": "hw4_questions (2).html#b.-latent-class-mnl",
    "href": "hw4_questions (2).html#b.-latent-class-mnl",
    "title": "Add Title",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current “wide” format into a “long” format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate – akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "hw4_questions (2).html#a.-k-nearest-neighbors",
    "href": "hw4_questions (2).html#a.-k-nearest-neighbors",
    "title": "Add Title",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\ntodo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm. The code generates a dataset with two features, x1 and x2, and a binary outcome variable y that is determined by whether x2 is above or below a wiggly boundary defined by a sin function.\n\n# gen data -----\nset.seed(42)\nn &lt;- 100\nx1 &lt;- runif(n, -3, 3)\nx2 &lt;- runif(n, -3, 3)\nx &lt;- cbind(x1, x2)\n\n# define a wiggly boundary\nboundary &lt;- sin(4*x1) + x1\ny &lt;- ifelse(x2 &gt; boundary, 1, 0) |&gt; as.factor()\ndat &lt;- data.frame(x1 = x1, x2 = x2, y = y)\n\ntodo: plot the data where the horizontal axis is x1, the vertical axis is x2, and the points are colored by the value of y. You may optionally draw the wiggly boundary.\ntodo: generate a test dataset with 100 points, using the same code as above but with a different seed.\ntodo: implement KNN by hand. Check you work with a built-in function – eg, class::knn() or caret::train(method=\"knn\") in R, or scikit-learn’s KNeighborsClassifier in Python.\ntodo: run your function for k=1,…,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?"
  },
  {
    "objectID": "hw4_questions (2).html#b.-key-drivers-analysis",
    "href": "hw4_questions (2).html#b.-key-drivers-analysis",
    "title": "Add Title",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  }
]